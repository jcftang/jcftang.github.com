
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Jimmy Tang</title>
  <meta name="author" content="Jimmy Tang">
  <meta name="Generator" content="Jekyll & Octopress (http://octopress.org)">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jcftang.github.com/">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/octopress.min.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Jimmy Tang" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-8597777-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


   
  <link href="/octopress-favicon.png" rel="icon">
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Jimmy Tang</a></h1>
  
    <h2>Yet more random mutterings</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jcftang.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
    










    <article >
      
  <header>
    <h1 class="entry-title">

<a href="/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/">Supercomputing 2012 and the Arms Race to Exascale</a>

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-11-16T06:30:00-07:00" pubdate data-updated="true">Nov 16<span>th</span>, 2012</time>
        
         | <a href="/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>It was clear at this year&#8217;s supercomputing conference that there wasn&#8217;t
as much excitement as previous years. There wasn&#8217;t much surprise as
nothing too revolutionary and radical was announced. In the past when
Bluegene/L and P arrived after the earth simulator there was an arms race
to being number 1 in the top500 list.  Even things like GPGPU&#8217;s aren&#8217;t
as cool anymore, everyone is selling effectively the same systems when
it comes to clusters. Not everyone has the budget to procure a specialist
machine like a NEC vector machine, a CRAY, Bluegene/Q etc&#8230;</p>

<p>The arms race to the top is just completely crazy, the top 50 or so
machines are so powerful compared to what a typical university or small
research lab might have access is too is completely skewed. At our site
we&#8217;re probably about 0.5% of the top machine. In the past we were about
1-2% typically of the top machine, and we were about 5yrs behind the
curve. The top 500 list should really be renamed to the top 50 list and
the green 500 list or the HPCC list should be used instead as a measure
of the top machines in the world. Do people really think that LINPACK
is a good measure of how powerful a machine is going to be?</p>

<p>What was interesting at this year&#8217;s conference was some of the papers
and panels were focused on project management and data management of
scientific datasets. I noticed that when people are starting to worry
about meta-data standards like librarians, people are starting to think
about archiving the data, they probably haven&#8217;t thought about the access
component of preservation which will be amusing when they do realise
it. There is also a disconnect from some of the papers which focused on
silent data corruption at the storage and network layers. The archiving
and preservation space for HPC will need to deal with bit-flips, silent
data corruption, malicious users and all that funk that is related to it.</p>

<p>After having a few chats with some vendors during the week, /me eyes
them vendors. There seems to be a bit of a lack of understanding that
archiving and preservation of data isn&#8217;t just about bulk, cheap and
reliable storage. There are apects to it which require data processing and
analytics of the data. It seems somewhat pointless to archive data and not
index it, process it and deliver it to who needs it. At somepoint the data
will be touched for checksumming, surrogate generation, delivery to the
end user. There also seems to be a lack of a guarantee of data-integrity
at the filesystem level. It appears that this responsiblity is left to
the application developer.</p>

<p>Would storage vendors (filesystems, devices the whole stack!) please
consider providing end to end data integrity? If the guys at the top are
trying to reach exascale computing in the next 5 years then the storage
component needs to catch up. Other sectors would also benefit from the
end to end data integrity built into storage systems.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    










    <article >
      
  <header>
    <h1 class="entry-title">

<a href="/2012/11/04/there-is-light-on-the-otherside/">There Is Light on the Otherside</a>

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-11-04T19:52:00-07:00" pubdate data-updated="true">Nov 4<span>th</span>, 2012</time>
        
         | <a href="/2012/11/04/there-is-light-on-the-otherside/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/11/04/there-is-light-on-the-otherside/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>Having spent the best part of my Sunday afternoon playing with ansible
just to learn and see what all the fuss is about, I was pleasantly
surprised with it.</p>

<p>I had installed <a href="https://github.com/ansible/ansible">ansible</a> on my OSX
laptop and <a href="https://github.com/ansible/vagrant-ansible">vagrant-ansible</a>
for my vagrant test environment.</p>

<p>The plan was to try and re-create my current ruby on rails development
and test virtual machine with vagrant. A secondary goal was to get it
to work with both Ubuntu Precise (LTS) and Scientific Linux 6.</p>

<p>My attempt at doing the above can be found at
<a href="https://github.com/jcftang/tchpc-vagrant/tree/ansible">tchpc-vagrant</a>
in the ansible branch. You will need ansible installed on your host
machine. You don&#8217;t need much installed in the target machine as ansible
is designed to login and execute commands as required, this is quite
refreshing. Compared to puppet and chef, if I were to roll this out
into production my overhead will be pretty low. This low overhead is
something that I really like as I don&#8217;t need to setup an infrastructure
just to run puppet.</p>

<p>In short I was able to learn how ansible is supposed to work (I think)
and build up enough configuration to start up a vagrant vm with what I
need for to do rails development in a matter of hours.</p>

<p>One thing that did occur to me was the lack of windows support, given
that ansible is designed to use ssh to carry out its activities, finding
stock windows machines which run ssh is pretty slim. This is one area
which puppet (perhaps chef too) is better at. It&#8217;s also one feature that
I would like as the vagrant vm&#8217;s that we&#8217;re using in work might be given
to windows users for testing and evaluation.</p>

<p>Going forward I think ansible will certainly be in my toolkit. There really is
a light on the otherside for mangement, deployment and orchestration.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/11/04/there-is-light-on-the-otherside/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    










    <article >
      
  <header>
    <h1 class="entry-title">

<a href="/2012/10/27/crowbar-for-deploying-systems/">Crowbar for Deploying Systems</a>

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-10-27T20:07:00-06:00" pubdate data-updated="true">Oct 27<span>th</span>, 2012</time>
        
         | <a href="/2012/10/27/crowbar-for-deploying-systems/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/10/27/crowbar-for-deploying-systems/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>I&#8217;ve been eyeing <a href="https://github.com/dellcloudedge/crowbar">crowbar</a>
recently, it looks pretty useful and interesting for deploying servers
and applications. I haven&#8217;t seen much if at all any documentation out
there which suggests that people in the digital preservation and archiving
fields are implementing systems at scale, I&#8217;m under the impression that
most systems/sites are building systems up one piece at a time without
much automation.</p>

<p>It seems to use <a href="http://www.opscode.com/chef/">chef</a> in the backend for
all the automation. I&#8217;ve been relearning <a href="http://puppetlabs.com/">puppet</a>
recently so that I can have reproducible environments with
<a href="http://vagrantup.com/">Vagrant</a>.</p>

<p>There might be an advantage to learn and port all the existing modules
that I have already created and configured to chef instead of puppet. If
I did move to a chef automation in my vagrant environments then a few
years from now when we go to full production we might be able to deploy
the whole system from bare metal relatively quickly and repeatably.</p>

<p>Automating the deployments will mean that we will have documentation
on the infrastructure itself. Either which way there is still a need
to automate the fedora-commons, SOLR, mysql and postgres deployments at
some point.</p>

<p>After all this thinking and pondering, I&#8217;m still using puppet. There&#8217;s
still the likes of ansible, cfengine, bcfg2 and juju. There is a never
ending supply of these tools.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/10/27/crowbar-for-deploying-systems/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    



  








    <article class="linklog">
      
  <header>
    <h1 class="entry-title">

<a href="http://ceph.com/releases/v0-53-released/">Ceph V0.53 RELEASED</a>
<span class='linklog-marker'>&rarr;</span>
</h1>

    
      <p class="meta">
        








  


<time datetime="2012-10-23T15:08:00-06:00" pubdate data-updated="true">Oct 23<span>rd</span>, 2012</time>
        
         | <a href="/2012/10/23/ceph-v0-dot-53-released/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/10/23/ceph-v0-dot-53-released/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>There&#8217;s a new release of Ceph, I hope that they release a stable soon so we can
do further evaluations of the Ceph storage
system. A few of my work colleagues are going to the <a href="http://www.inktank.com/news-events/event/ceph-workshops-amsterdam/">Ceph
workshop</a>
next week.</p>

<p>I&#8217;m wondering if anyone has taken the CRUSH algorithm and used it in other
domains.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/10/23/ceph-v0-dot-53-released/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    



  








    <article class="linklog">
      
  <header>
    <h1 class="entry-title">

<a href="http://yourmediashelf.com/hydracamp/">Hydracamp 2012 - Penn State</a>
<span class='linklog-marker'>&rarr;</span>
</h1>

    
      <p class="meta">
        








  


<time datetime="2012-10-13T07:34:00-06:00" pubdate data-updated="true">Oct 13<span>th</span>, 2012</time>
        
         | <a href="/2012/10/13/hydracamp-2012-penn-state/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/10/13/hydracamp-2012-penn-state/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>What do you do when you need a crash course on RoR, Hydra and frameworks for
digital preservation and archiving? You go to Hydracamp!</p>

<p>The syllabus was</p>

<ul>
<li>Day 1 - Rails, CRUD, TDD and Git</li>
<li>Day 2 - Collaborative development with Stories, Tickets, TDD and Git</li>
<li>Day 3 - Hydra, Fedora, XML and RDF (ActiveFedora and OM)</li>
<li>Day 4 - SOLR and Blacklight</li>
<li>Day 5 - Hydra-head, Hydra Access Controls</li>
</ul>


<p>Most of the training sessions were hands on from day 1 which was
refreshing, as it was hands on I getting the most out of the training
session. It would have been better if I had known more ruby to move
along some of the exercises more effectively.</p>

<p>To give an overview of what we had done (between ~30 people), we created
a ruby on rails application titled &#8220;Twitter for Zombies&#8221;. With this small
application everybody was frantically committing, pulling, merging and
pushing code. It was highly informative and a good learning experience
to see how fast things could move.</p>

<p>The training session also included a crash course into what Fedora and
SOLR does and how Hydra interacts with these components. The third and
fourth days were the most interesting as it showed how someone might
convert from a typical RoR application into an application which uses
Fedora as the persistance layer. The last day was really just a wrap up
and Q&amp;A session.</p>

<p>You could take a look at the <a href="https://github.com/projecthydra">github</a>
account for Project Hydra and have a peek at the hydracamp repo.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/10/13/hydracamp-2012-penn-state/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    










    <article >
      
  <header>
    <h1 class="entry-title">

<a href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/">Digital Preservation and Archiving Is a HPC Problem?</a>

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-10-07T13:42:00-06:00" pubdate data-updated="true">Oct 7<span>th</span>, 2012</time>
        
         | <a href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>I shall be going to SC2012 next month, I plan on hitting a few of the
storage vendors for possible collaborations and flagging to them that
we&#8217;re on the look out for storage systems. One of the first
observation that the reader will note is &#8220;where is that link between
HPC and Digital Preservation and Archiving&#8221;. It&#8217;s probably not obvious
to most people, one of the big problems in the area of preservation
and archiving is the the amount of data involved and the varied types
of data. This is not taking into account of the issues with data
access patterns.</p>

<p>Given that a preservation and archiving project will want to provide a
trusted system, the system will want to read out every single byte
that was put in to verify that the data is correct at somepoint
(usually with some form of hashing).</p>

<p>Reading data out and checking that it&#8217;s correct serially probably
isn&#8217;t the smartest solution. Nor is copying the data into 2-3
locations (where each site is maintaining 2-3 copies for backups and
redundancy). The current and seemingly most popular solutions is to
dump the data to a few offsite locations (such as S3 or SWIFT)
compatible storage systems, then just hoping for the best that if
anyone of the sites is down or corrupted there site can be restored
from the other sites or from a backup. I need to delve deeper into the
storage and data-distribution strategies that some of the bigger
projects are taking. There has to be a smarter way of storing and
preserving data without having to make copies of things.</p>

<p>I&#8217;ve often wondered how projects manage to copy/move data across
storage providers in a reasonable amount of time without needing to
wheel a few racks of disks around. It would also be interesting to see
the error rates of these systems and how often errors are
corrected. If they are corrected what is the computational cost of
doing this.</p>

<p>If you have a multi-terabyte archive the problem isn&#8217;t too bad, the
more typical case these days might be in the order of the low hundreds
of terabytes. I could only imagine what lager scale sites must deal
with. I&#8217;m still not a fan of moving a problem from a local site to a
remote site as it often shows that there is a lack of understanding to
the problem. Storage in the preservation and archiving domain will
probably turn into an IO and compute intensive operation at some
point, especially if you want to do something with the data.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    



  








    <article class="linklog">
      
  <header>
    <h1 class="entry-title">

<a href="https://github.com/jcftang/slurm-bank">SLURM-Bank That Big Script for Banking in SLURM</a>
<span class='linklog-marker'>&rarr;</span>
</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-29T19:52:00-06:00" pubdate data-updated="true">Sep 29<span>th</span>, 2012</time>
        
         | <a href="/2012/09/29/slurm-bank-that-big-script-for-banking-in-slurm/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/29/slurm-bank-that-big-script-for-banking-in-slurm/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>A co-worker of mine (Paddy Doyle) had originally hacked at a perl script
for reporting balances from SLURM&#8217;s accounting system a year or two ago
and he had figured out that it might be possible to do some minimalistic
&#8216;configuration&#8217; and scripting to get a system that&#8217;s very basic but
functional.</p>

<p>It was just one of those things that funding agencies wanted to justify
how the system was being used, GOLD was clunky and obtrusive and
complicated for what we wanted. Most of all we liked SLURM but not GOLD
and Maui which was needed to get full accounting and banking (most of
the features weren&#8217;t used).</p>

<p>Being good and lazy engineers we got excited with the prospect of having
the option of replacing SLURM, Maui and GOLD with just plain old SLURM
we set out to write down the workflows for what we wanted to do and what
the user and funding agencies actually wanted. With those ideas in mind
we set out to implement as much as we could and needed in just plain
old sh/bash scripting with a splash of perl. Replacing two components
with one meant that we would have less work to do in the long run ;)</p>

<p>After a whole year of running with these scripts and just putting it
online, I&#8217;ve noticed that there may be a few sites out there that might
be using our scripts and workflows. It would be nice to find out how
many people are using our implementation of a banking system in SLURM
and if it&#8217;s driven by sysadmins looking to account for usage or is it
funding agencies looking for justification of the usage of a system.</p>

<p>I was going to be at the SLURM User Group Meeting 2012 to give a
short talk on our experiences with the SLURM-Bank scripts and workflow,
but sadly I have to be in the US during this meeting and my colleague
&#8220;Paddy Doyle&#8221; will there instead of me.  I would have liked to go and chat
with the developers of SLURM to push for more advanced banking/accounting
facilities in SLURM itself. Visiting BSC again would have been fun.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/09/29/slurm-bank-that-big-script-for-banking-in-slurm/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    



  








    <article class="linklog">
      
  <header>
    <h1 class="entry-title">

<a href="http://ceph.com/releases/v0-52-released/">Ceph V0.52 RELEASED</a>
<span class='linklog-marker'>&rarr;</span>
</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-28T15:52:00-06:00" pubdate data-updated="true">Sep 28<span>th</span>, 2012</time>
        
         | <a href="/2012/09/28/ceph-v0-dot-52-released/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/28/ceph-v0-dot-52-released/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>The latest development branch of Ceph is out with some rather nice
looking features, what&#8217;s probably the most useful are the RPM builds
for those that run RHEL6 like systems.</p>

<p>Still no real sight of backported kernel modules :P Also some of the
guys in work here just deployed a ~200tb Ceph installation which I&#8217;ve
access to a 10tb RBD for doing backups on.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/09/28/ceph-v0-dot-52-released/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    










    <article >
      
  <header>
    <h1 class="entry-title">

<a href="/2012/09/23/a-poor-mans-nas-device-with-ceph/">A Poor Man&#8217;s NAS Device With Ceph</a>

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-23T08:59:00-06:00" pubdate data-updated="true">Sep 23<span>rd</span>, 2012</time>
        
         | <a href="/2012/09/23/a-poor-mans-nas-device-with-ceph/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/23/a-poor-mans-nas-device-with-ceph/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>Given that I have a number of old 64bit capable desktop machines and a
collection of hard drives at home, I could have run
<a href="https://tahoe-lafs.org/trac/tahoe-lafs">Tahoe-LAFS</a> like I do in work
for backup purposes. In fact Tahoe works quite well for the
technically capable user.</p>

<p>Recently I&#8217;ve decided that I need a more central location at home to
store my photo collection (I love to take photos with my Canon DSLR
and Panasonic LX5). Traditionally I would have just fired up
<a href="http://git-annex.branchable.com/">git-annex</a> to track the data and
then setup a number of remotes to store the data, where one of them
might be Tahoe-LAFS and the rest might be portable hard drives, remote
machines etc&#8230;</p>

<p>I could have gone with any number of distributed storage solutions
such as <a href="http://www.gluster.org/">GlusterFS</a>,
<a href="http://www.irods.org">iRODS</a>,
<a href="http://xrootd.slac.stanford.edu/">xrootd</a>,
<a href="http://wiki.lustre.org/index.php/Main_Page">Lustre</a> or
<a href="http://www.xtreemfs.org/">xtreemfs</a>. I&#8217;ve worked with some of these
systems in production and toyed with others. Since this is for a home
system I can pick what I want and change it at will.</p>

<p>I probably have 2-3tb&#8217;s of data to archive and store, I also want easy
access to my data so NFS or CIFS exports are required. It wouldn&#8217;t be
unfeasible to acquire a few 2 or 3 terabyte drives for my old desktop
machine which would effectively provide me with a 2 or 3 terabyte
replicated data store. Given the amount of toying around and learning
about Ceph in my spare time I would expect that Ceph would provide me
with a pretty good &#8220;backend&#8221; system for storing my files and the
option of &#8220;migrating my data from one machine to another machine&#8221; by
adding and removing OSD&#8217;s. The handiest feature for me will be the
capability of expanding and shrinking the system as I need.</p>

<p>There probably aren&#8217;t many people who would want to setup something
like this for a home system, but it is an alternative to the usual
RAID or LVM setup.</p>

<p>Here&#8217;s my proposed setup which I&#8217;m going to setup in the next few
spare weekends that I will have.</p>

<p><img class="" src="/downloads/images/ceph-home.png"></p>

<p>It would be great if Ceph offered some of of parity/erasure coding
instead of plain replication. I&#8217;m greedy and I want to maximise my
disks that I have, I wonder how low I can go on hardware with the Ceph
software.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/09/23/a-poor-mans-nas-device-with-ceph/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
    



  








    <article class="linklog">
      
  <header>
    <h1 class="entry-title">

<a href="http://ceph.com/releases/v0-48-2-argonaut-stable-update-released/">Ceph V0.48.2 ARGONAUT RELEASED</a>
<span class='linklog-marker'>&rarr;</span>
</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-21T10:38:00-06:00" pubdate data-updated="true">Sep 21<span>st</span>, 2012</time>
        
         | <a href="/2012/09/21/ceph-v0-dot-48-dot-2-argonaut-released/#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/21/ceph-v0-dot-48-dot-2-argonaut-released/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>There&#8217;s a new stable release of Ceph Argonaut, I seem to be having better
luck with playing with the development releases of Ceph.</p>

<p>Oh how I wish that there was a backport of the kernel ceph and rbd drivers
for RHEL6, I have a dodgy repo and some reverted commits that one of
the guys in work told me about. It seems to run but it isn&#8217;t great,
it can be found at <a href="https://github.com/jcftang/ceph-client-standalone">https://github.com/jcftang/ceph-client-standalone</a>.</p>
</div>

  <footer>
    
    <p><a class="comments-link" href="/2012/09/21/ceph-v0-dot-48-dot-2-argonaut-released/#disqus_thread">View comments &raquo;</a></p>
    
    
  </footer>


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/archives/">Blog Archives</a>
    
  </div>
</div>

<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>This is more of a technical blog of what I want to rant about
  separate from my <a href="http://www.sgenomics.org/~jtang/">personal blog</a>.</p>
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/">Supercomputing 2012 and the arms race to exascale</a>
      </li>
    
      <li class="post">
        <a href="/2012/11/04/there-is-light-on-the-otherside/">There is light on the otherside</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/27/crowbar-for-deploying-systems/">Crowbar for deploying systems</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/23/ceph-v0-dot-53-released/">Ceph V0.53 RELEASED</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/13/hydracamp-2012-penn-state/">Hydracamp 2012 - Penn State</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>On GitHub</h1>
  <ul id="gh_repos" data-user="jcftang" data-count="0" data-skip="true">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a class="github-follow" href="https://github.com/jcftang">Follow @jcftang</a>
  
</section>



<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets" data-user="jcftang" data-count="4" data-replies="false">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
    <a href="//twitter.com/jcftang" class="twitter-follow-button" data-show-count="false">Follow @jcftang</a>
  
</section>







  <a href="https://plus.google.com/115375964029787125148?rel=author">
    <img src="https://ssl.gstatic.com/images/icons/gplus-32.png" alt="Google Plus icon">
  </a>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Jimmy Tang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jcftang';
			var disqus_developer = '0';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>






</body>
</html>
