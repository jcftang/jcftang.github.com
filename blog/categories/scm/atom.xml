<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: scm | Jimmy Tang]]></title>
  <link href="http://jcftang.github.com/blog/categories/scm/atom.xml" rel="self"/>
  <link href="http://jcftang.github.com/"/>
  <updated>2012-02-13T08:45:20+00:00</updated>
  <id>http://jcftang.github.com/</id>
  <author>
    <name><![CDATA[Jimmy Tang]]></name>
    <email><![CDATA[jcftang@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[cports for building applications and libraries for HPC systems]]></title>
    <link href="http://jcftang.github.com/blog/2012/02/12/cports-for-building-applications-and-libraries-for-hpc-systems/"/>
    <updated>2012-02-12T17:59:00+00:00</updated>
    <id>http://jcftang.github.com/blog/2012/02/12/cports-for-building-applications-and-libraries-for-hpc-systems</id>
    <content type="html"><![CDATA[<p>I've talked about <em>cports</em> in the past, it's basically a collection of
<em>makefiles</em> which mostly automates the process of downloading,
configuring, building and installing applications and libraries for
High Performance Computing systems that use environment-modules.</p>

<p>One of the key-features that <em>cports</em> offers is the automated
modulefile generation, and the fact that the <em>makefiles</em> acts as
documentation to how software is configured, built and installed. It's
currently being used on the clusters at my work place, it has been a
boost to the productivity of the systems admin team. It's a nice
alternative to trying to create RPM's or DEB's (pick your custom
package manager of choice here), as <em>makefiles</em> tend to be a little
more flexible than traditional package managers.</p>

<p>One main drawback of the <em>cports</em> system right now is the lack of good
dependancy management and checking, it is all currently up to the
packager to resolve these dependancy issues. It's also <em>cports</em>
strongest point that there is no dependancy management, as the
packager can build many unique trees of packages.</p>

<p>For example, we have Tahoe-LAFS in the following sample makefile</p>

<p>```
include ../../../mk/gnu.pre.mk</p>

<p>DISTNAME=   allmydata-tahoe
VERSION=    1.9.0-SUMO
CATEGORIES= system
HOMEPAGE=   http://tahoe-lafs.org/
MASTER_SITES=   http://tahoe-lafs.org/source/tahoe-lafs/releases/
MAINTAINER= jtang@tchpc.tcd.ie
HAS_CONFIGURE=  no
DISTFILES = $(DISTNAME)-$(VERSION).tar.bz2</p>

<p>DEPENDS="Python/2.6.5 --build-env Python/2.6.5 --run-env"
DEPENDS+="openssl/0.9.8o --lib --build-env"
DEPENDS+="gmp/4.3.2 --lib --build-env"</p>

<p>DESCRIPTION=    "Tahoe-LAFS (Least Authority File System) is a Free Software/Open Source"
DESCRIPTION+=    "decentralized data store. It distributes your filesystem across multiple"
DESCRIPTION+=    "servers, and even if some of the servers fail or are taken over by"
DESCRIPTION+=    "an attacker, the entire filesystem continues to work correctly and to"
DESCRIPTION+=    "preserve your privacy and security."</p>

<p>CONFIGURE_ARGS +=</p>

<p>MODULEFILE_LINES+=      PYTHONPATH
MODULEFILE_CMD_PYTHONPATH?= \</p>

<pre><code>    $(ECHO) "prepend-path   PYTHONPATH $(PROGRAM_PREFIX)/lib/python2.6/site-packages";
</code></pre>

<p>do-build:</p>

<pre><code>$(MODULE_ADD) $(BUILD_DEPENDS); \
cd $(WRKSRC); \
</code></pre>

<p>do-install:</p>

<pre><code>$(MODULE_ADD) $(BUILD_DEPENDS); \
cd $(WRKSRC); \
$(MKDIR) $(PROGRAM_PREFIX) ;\
tar -cpf - . | (cd $(PROGRAM_PREFIX)/ &amp;&amp; tar -xpf - ) ;\
cd $(PROGRAM_PREFIX) ;\
cp -a tahoe-deps ../ ;\
python setup.py build ;
</code></pre>

<p>do-test:</p>

<pre><code>$(MODULE_ADD) $(RUN_DEPENDS) $(DISTNAME)/$(VERSION)$(EXTRAVERSION)$(COMPILER_TAG) ; \
cd $(WRKSRC); \
python setup.py test
</code></pre>

<p>include ../../../mk/gnu.post.mk
```</p>

<p>In this example, as Tahoe-LAFS (upstream project) gets updated, the
cports packager just needs to copy this makefile to a new directory,
update the version numbers, then do a <em>make install</em>. This assumes
that the dependancies haven't changed much, if they have it is trivial
to update the dependancies. The above example generates a modulefile
similar to like this,</p>

<p>```</p>

<h1>%Module1.0</h1>

<p>module-whatis "allmydata-tahoe version 1.9.0-SUMO (compiled with a gnu compiler)"
conflict allmydata-tahoe
prepend-path   PYTHONPATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/lib
/python2.6/site-packages
prepend-path PATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/bin
module add Python/2.6.5-gnu
proc ModulesHelp { } {
puts stderr "Tahoe-LAFS (Least Authority File System) is a Free Software/Open Source"
puts stderr "decentralized data store. It distributes your filesystem across multiple"
puts stderr "servers, and even if some of the servers fail or are taken over by"
puts stderr "an attacker, the entire filesystem continues to work correctly and to"
puts stderr "preserve your privacy and security."
puts stderr {build depends: gmp/4.3.2-gnu openssl/0.9.8o-gnu Python/2.6.5-gnu}
puts stderr {run depends: Python/2.6.5-gnu}
puts stderr {module depends: gmp/4.3.2 openssl/0.9.8o Python/2.6.5}
puts stderr {link depends: gmp/4.3.2-gnu openssl/0.9.8o-gnu}
}
prepend-path   PYTHONPATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/lib
/python2.6/site-packages
```</p>

<p>Once the package has been built and tested on a development system, we
can take the package and replicate the installation in fairly
automated fashion. This type of scripting and automation means that
the clusters that we run in work have consistent installations of
applications.</p>

<p>Having consistent installs means that the end-user needs to learn less
about the naming conventions. This in turn reduces the confusion and
documentation that is needed for the end user.</p>

<p>Sadly, I don't get to play with <em>cports</em> as much as I used to, since I
do not administrate High Performance Computing systems anymore in my
new role at my current work place. It is just a hobby to develop this
build system, I plan on automating more testing of the <em>cports</em> system
when I get a chance. We currently have <em>jenkins</em> and <em>gitbuilder</em>
running on select machines in work to continually build and test
specific packages to find regressions and broken download links.</p>

<p><em>cports</em> isn't quite ready for general public usage, but if you are a
clued in systems administrator at a High Performance Computing
facility and use environment-module then <em>cports</em> is just about
usable. This is of course if you are willing to look at the sample
packages and write <em>makefiles</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using continuous integration systems in a team]]></title>
    <link href="http://jcftang.github.com/blog/2011/12/13/using-continuous-integration-systems-in-a-team/"/>
    <updated>2011-12-13T07:37:00+00:00</updated>
    <id>http://jcftang.github.com/blog/2011/12/13/using-continuous-integration-systems-in-a-team</id>
    <content type="html"><![CDATA[<p>Apart from the obvious unit testing code which could be fun trying to
convince a team to use. There are things known as 'Continuous
Integration' processes and servers. The basic idea is to continually
build and test your product automatically and report on successful and
failed builds.</p>

<p>I've known about this methodology for long time now but I've never
bothered to install a CI server since it was always for myself. In the
past a loop in a shell script running make and or make test usually
did the trick. Then I discovered gitbuilder which is just a small set
of scripts for automating the process. Today I have Jenkins installed
and I'm somewhat looking at buildbot as well.</p>

<p>Jenkins appears to be the most responsive and feature rich of the
three systems that I have played with. Buildbot looks lightweight and
scalable.  Though I do miss the git bisect and build on a failure that
gitbuilder provides.</p>

<p>Having to work with a team that is geographically distributed
sometimes makes idea exchanges, organising events and development
difficult. Any tool that I can get my hands on to encourage
collaborative work and discussion is going to be used.</p>

<p>Automating the build and test process also means that it is possible
to automate the release process. If all the 'tests' pass then why not
release it? I will need to look at integrating things with fitnesse
next. Other benefits from using a CI process or server means that most
if not all things get automated, this turns into a highly valuable
source of documentation for incoming developers and
engineers. Documentation often gets left to the end instead of being
done on a continual basis as the project progresses.</p>

<p>Hopefully the CI server won't be used to name and shame developers in
the group, it's there to make sure everything is working as expected.</p>

<p>Now to convince the team that building early and continually is
actually a good idea! Having somewhat successfully kicked off some
pairing sessions with various team members is good, but being more
flexible and professional about what we are building is what I'm
after.</p>
]]></content>
  </entry>
  
</feed>
