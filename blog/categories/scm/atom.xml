<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: scm | Jimmy Tang]]></title>
  <link href="http://jcftang.github.com/blog/categories/scm/atom.xml" rel="self"/>
  <link href="http://jcftang.github.com/"/>
  <updated>2012-04-01T11:47:19+01:00</updated>
  <id>http://jcftang.github.com/</id>
  <author>
    <name><![CDATA[Jimmy Tang]]></name>
    <email><![CDATA[jcftang@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      




<title type="html"><![CDATA[Gitbuilder aggregator &rarr;]]></title>
<link href="https://github.com/jcftang/gitbuilder/tree/develop/contrib/gitbuilder-ajax"/>
<updated>2012-04-01T11:05:00+01:00</updated>
<id>http://jcftang.github.com/blog/2012/04/01/gitbuilder-aggregator</id>

      <content type="html"><![CDATA[<p>We use git and <a href="https://github.com/apenwarr/gitbuilder">gitbuilder</a> in
work for a large number of projects, we also try and test things as
much as we can. I first noticed that someone had written an aggregator
for gitbuilder at <a href="http://ceph.newdream.net/gitbuilder.cgi">ceph
gitbuilders</a>, this seemed
like a great idea (and it is) except the aggregator at the time didn't
quite work very fast and needed some ajax magic.</p>

<p>I had asked for a copy of the aggregator script from the ceph
developers, this was really just a perl hack as they said, but it
works. Since we had some students doing an internship here to learn
new things, I got one of the interns to write an ajax'd up version of
the aggregator.</p>

<p>After a few weeks worth of usage and minor changes, it's a bit more
ready to share with everyone, the ajax'd up version of the aggregator
can be found at my
<a href="https://github.com/jcftang/gitbuilder/tree/develop/contrib/gitbuilder-ajax">github</a>
account in the develop branch. For fun I updated the main gitbuilder
cgi scripts to use twitter bootstrap and add a link to the errcache
file that gitbuilder generates.</p>

<p>We found that with large builds the logs would just swamp out the
errors and warnings and having access to the errcache helped a lot in
narrowing down where to look for problems, hence the linking to the
errcache.</p>

<p>At somepoint it might be worth re-implementing the gitbuilder scripts
in a single language in a generic way such that it works with other
DVCS's that have the bisect feature.<p><a rel="bookmark" href="http://jcftang.github.com/blog/2012/04/01/gitbuilder-aggregator/">&infin; Permalink</a></p></p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Dogfooding your own project to accelerate development]]></title>
<link href="http://jcftang.github.com/blog/2012/03/15/dogfooding-your-own-project-to-accelerate-development/"/>
<updated>2012-03-15T19:31:00+00:00</updated>
<id>http://jcftang.github.com/blog/2012/03/15/dogfooding-your-own-project-to-accelerate-development</id>

      <content type="html"><![CDATA[<p>Should you dogfood your own project that you are developing? The
answer is probably yes, especially if you have no clear cut
requirements from the stakeholder in a project with a greenfield for
development. There is a lot to be said about having a working
implementation that can be presented and refined.</p>

<p>Sometimes the project that you are working on won't have clear
requirements for implementation, so you should probably take basic
assumed cases and run with it. Starting early to see what works and
what doesn't work is a pragmatic approach which the waterfall crowd
might not like. But hey, an implementation speaks for itself.</p>

<p>If you don't use what you develop, then it is very hard to relate to
the customer/end-user in the long run. On the note of dogfooding your
own work, sometimes best-practice might cost too much in terms of time
and money, sometimes good-enough practice might just be enough to
deliver a functioning product.</p>

<p>Accelerating development by dogfooding your own work and using
good-enough practices should increase throughput of development, but
not necessarily quality. Again in a greenfield project where there
aren't many requirements due to the schedule, it's worth taking this
approach until hard requirements get delivered.</p>

<p><a rel="bookmark" href="http://jcftang.github.com/blog/2012/03/15/dogfooding-your-own-project-to-accelerate-development/">&infin; Permalink</a></p>

]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[git rerere for long lived feature branches &rarr;]]></title>
<link href="http://progit.org/2010/03/08/rerere.html"/>
<updated>2012-03-11T11:45:00+00:00</updated>
<id>http://jcftang.github.com/blog/2012/03/11/git-rerere-for-long-lived-feature-branches</id>

      <content type="html"><![CDATA[<p>I turned this feature on for a few of my git repos but I had
completely forgotten about it. As far as I recall the feature has been
around for a few years now. It can be turned on globally by doing</p>

<pre><code>git config --global rerere.enabled 1
</code></pre>

<p>It pretty much automates the resolution of conflicts in long lived
branches. I've been lazy recently and I have just doing merges instead
rebasing, which lead me to re-discover <em>git rerere</em>.</p>

<p><a rel="bookmark" href="http://jcftang.github.com/blog/2012/03/11/git-rerere-for-long-lived-feature-branches/">&infin; Permalink</a></p>

]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Why Linux isn't the only platform to target when building applications]]></title>
<link href="http://jcftang.github.com/blog/2012/02/16/why-linux-isnt-the-only-platform-to-target-when-building-applications/"/>
<updated>2012-02-16T09:33:00+00:00</updated>
<id>http://jcftang.github.com/blog/2012/02/16/why-linux-isnt-the-only-platform-to-target-when-building-applications</id>

      <content type="html"><![CDATA[<p>Why would one want to target other platforms when building applications
on the server side?</p>

<p>This came out of a conversation with the ex-CTO of Creme software (he
is also a friend of mine), the conversation started out with why I
like to use Macs and OSX as my laptop or workstation. I've been a long
time Linux user of pretty much most of the major distributions ranging
from RHEL, Debian/Ubuntu, Gentoo, ArchLinux as well as a number of
other derivatives, not to mention other systems like the BSD's which I
have a soft spot for.</p>

<p>I interchange the terms Linux and Distros quite a bit in this post.</p>

<p>Some of the things that didn't like with the Linux's was that not all
my hardware would be supported all the time, the distro's sometimes
think that it's a good idea to completely change how lowlevel systems
work in favour of <em>what's hot right now</em>, sometimes the lack of long
term support for security updates (not package updates to fix security
problems) does make it more difficult to plan and deploy. The
perceived flexibility sometimes causes headaches with migration plans
and maintanence.</p>

<p>Of course there are things that I like, the access to the source code
and packaging to fix and redeploy packages. The stability and
reliability that can be achieved is attractive if everything is
automated and planned out (though too much automation can be bad too).</p>

<p>To get back to the original point of why you wouldn't want to target
Linux specifically when building applications? Unless you have a
strong motivating reason to write code that <em>specifically</em> requires a
feature of Linux (or any other operating system), then you really
ought to be writing code that adheres to at least some POSIX or cross
platform standard, and pick libraries that are known to have good
cross platform compatibility. There is nothing to gain from writing
platform specific codes in the long run, the platform might hide
issues from the developer if it is too clever. Linux or more
specifically the distros might change various behaviour of the
underlying system, and if your code is tied down to particular
features of the underlying system then you will have lots of fun
migrating.</p>

<p>It's just bad practice to rely on system specific behaviour which
isn't portable (or maintainable) going forward in a project. To
mitigate some of this, one would want to at least try to use a
continuous build systems such as <em>gitbuilder</em>, <em>buildbot</em> or <em>jenkins</em>
on a bunch of <strong>different</strong> architectures and platforms. This will
reveal portability issues and more often than not, subtle bugs in your
code which you probably didn't see as a result of your development
system being too smart for you!</p>

<p>There isn't much of an excuse not to do continuous builds and testings
across different Linux, BSD and Solaris distributions these
days. Diskspace and compute power is cheap, there are free and
opensource virtualisation technologies out there to provide you with a
means to run different distributions for testing on a single
machine. The problem will be the upfront manpower needed to setup such
a system.</p>

<p>In the long run targetting at least two platforms will make your code
base far more portable and hopefully more maintainable as you will end
up making sure you write code once that runs on many systems with
minimal changes needed when a new platform arises.</p>

<p><a rel="bookmark" href="http://jcftang.github.com/blog/2012/02/16/why-linux-isnt-the-only-platform-to-target-when-building-applications/">&infin; Permalink</a></p>

]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[cports for building applications and libraries for HPC systems &rarr;]]></title>
<link href="http://thammuz.tchpc.tcd.ie/mirrors/cports/releases/"/>
<updated>2012-02-12T17:59:00+00:00</updated>
<id>http://jcftang.github.com/blog/2012/02/12/cports-for-building-applications-and-libraries-for-hpc-systems</id>

      <content type="html"><![CDATA[<p>I've talked about <em>cports</em> in the past, it's basically a collection of
<em>makefiles</em> which mostly automates the process of downloading,
configuring, building and installing applications and libraries for
High Performance Computing systems that use environment-modules.</p>

<p>One of the key-features that <em>cports</em> offers is the automated
modulefile generation, and the fact that the <em>makefiles</em> acts as
documentation to how software is configured, built and installed. It's
currently being used on the clusters at my work place, it has been a
boost to the productivity of the systems admin team. It's a nice
alternative to trying to create RPM's or DEB's (pick your custom
package manager of choice here), as <em>makefiles</em> tend to be a little
more flexible than traditional package managers.</p>

<p>One main drawback of the <em>cports</em> system right now is the lack of good
dependancy management and checking, it is all currently up to the
packager to resolve these dependancy issues. It's also <em>cports</em>
strongest point that there is no dependancy management, as the
packager can build many unique trees of packages.</p>

<p>For example, we have Tahoe-LAFS in the following sample makefile</p>

<p>```
include ../../../mk/gnu.pre.mk</p>

<p>DISTNAME=   allmydata-tahoe
VERSION=    1.9.0-SUMO
CATEGORIES= system
HOMEPAGE=   http://tahoe-lafs.org/
MASTER_SITES=   http://tahoe-lafs.org/source/tahoe-lafs/releases/
MAINTAINER= jtang@tchpc.tcd.ie
HAS_CONFIGURE=  no
DISTFILES = $(DISTNAME)-$(VERSION).tar.bz2</p>

<p>DEPENDS="Python/2.6.5 --build-env Python/2.6.5 --run-env"
DEPENDS+="openssl/0.9.8o --lib --build-env"
DEPENDS+="gmp/4.3.2 --lib --build-env"</p>

<p>DESCRIPTION=    "Tahoe-LAFS (Least Authority File System) is a Free Software/Open Source"
DESCRIPTION+=    "decentralized data store. It distributes your filesystem across multiple"
DESCRIPTION+=    "servers, and even if some of the servers fail or are taken over by"
DESCRIPTION+=    "an attacker, the entire filesystem continues to work correctly and to"
DESCRIPTION+=    "preserve your privacy and security."</p>

<p>CONFIGURE_ARGS +=</p>

<p>MODULEFILE_LINES+=      PYTHONPATH
MODULEFILE_CMD_PYTHONPATH?= \</p>

<pre><code>    $(ECHO) "prepend-path   PYTHONPATH $(PROGRAM_PREFIX)/lib/python2.6/site-packages";
</code></pre>

<p>do-build:</p>

<pre><code>$(MODULE_ADD) $(BUILD_DEPENDS); \
cd $(WRKSRC); \
</code></pre>

<p>do-install:</p>

<pre><code>$(MODULE_ADD) $(BUILD_DEPENDS); \
cd $(WRKSRC); \
$(MKDIR) $(PROGRAM_PREFIX) ;\
tar -cpf - . | (cd $(PROGRAM_PREFIX)/ &amp;&amp; tar -xpf - ) ;\
cd $(PROGRAM_PREFIX) ;\
cp -a tahoe-deps ../ ;\
python setup.py build ;
</code></pre>

<p>do-test:</p>

<pre><code>$(MODULE_ADD) $(RUN_DEPENDS) $(DISTNAME)/$(VERSION)$(EXTRAVERSION)$(COMPILER_TAG) ; \
cd $(WRKSRC); \
python setup.py test
</code></pre>

<p>include ../../../mk/gnu.post.mk
```</p>

<p>In this example, as Tahoe-LAFS (upstream project) gets updated, the
cports packager just needs to copy this makefile to a new directory,
update the version numbers, then do a <em>make install</em>. This assumes
that the dependancies haven't changed much, if they have it is trivial
to update the dependancies. The above example generates a modulefile
similar to like this,</p>

<p>```</p>

<h1>%Module1.0</h1>

<p>module-whatis "allmydata-tahoe version 1.9.0-SUMO (compiled with a gnu compiler)"
conflict allmydata-tahoe
prepend-path   PYTHONPATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/lib
/python2.6/site-packages
prepend-path PATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/bin
module add Python/2.6.5-gnu
proc ModulesHelp { } {
puts stderr "Tahoe-LAFS (Least Authority File System) is a Free Software/Open Source"
puts stderr "decentralized data store. It distributes your filesystem across multiple"
puts stderr "servers, and even if some of the servers fail or are taken over by"
puts stderr "an attacker, the entire filesystem continues to work correctly and to"
puts stderr "preserve your privacy and security."
puts stderr {build depends: gmp/4.3.2-gnu openssl/0.9.8o-gnu Python/2.6.5-gnu}
puts stderr {run depends: Python/2.6.5-gnu}
puts stderr {module depends: gmp/4.3.2 openssl/0.9.8o Python/2.6.5}
puts stderr {link depends: gmp/4.3.2-gnu openssl/0.9.8o-gnu}
}
prepend-path   PYTHONPATH /home/support/apps/cports/rhel-5.x86_64/gnu/allmydata-tahoe/1.9.0-SUMO/lib
/python2.6/site-packages
```</p>

<p>Once the package has been built and tested on a development system, we
can take the package and replicate the installation in fairly
automated fashion. This type of scripting and automation means that
the clusters that we run in work have consistent installations of
applications.</p>

<p>Having consistent installs means that the end-user needs to learn less
about the naming conventions. This in turn reduces the confusion and
documentation that is needed for the end user.</p>

<p>Sadly, I don't get to play with <em>cports</em> as much as I used to, since I
do not administrate High Performance Computing systems anymore in my
new role at my current work place. It is just a hobby to develop this
build system, I plan on automating more testing of the <em>cports</em> system
when I get a chance. We currently have <em>jenkins</em> and <em>gitbuilder</em>
running on select machines in work to continually build and test
specific packages to find regressions and broken download links.</p>

<p><em>cports</em> isn't quite ready for general public usage, but if you are a
clued in systems administrator at a High Performance Computing
facility and use environment-module then <em>cports</em> is just about
usable. This is of course if you are willing to look at the sample
packages and write <em>makefiles</em>.</p>

<p><a rel="bookmark" href="http://jcftang.github.com/blog/2012/02/12/cports-for-building-applications-and-libraries-for-hpc-systems/">&infin; Permalink</a></p>

]]></content>
    </entry>
  
</feed>

