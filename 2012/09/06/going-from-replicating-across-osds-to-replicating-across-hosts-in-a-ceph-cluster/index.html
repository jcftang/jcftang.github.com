
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Going from replicating across OSD's to replicating across hosts in a Ceph cluster - Jimmy Tang</title>
  <meta name="author" content="Jimmy Tang">
  <meta name="Generator" content="Jekyll & Octopress (http://octopress.org)">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jcftang.github.com/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/octopress.min.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Jimmy Tang" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-8597777-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


   
  <link href="/octopress-favicon.png" rel="icon">
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Jimmy Tang</a></h1>
  
    <h2>Yet more random mutterings</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jcftang.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>











<article class="hentry " role="article">
  
  <header>
    <h1 class="entry-title">

Going From Replicating Across OSD's to Replicating Across Hosts in a Ceph Cluster

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-06T21:44:00+01:00" pubdate data-updated="true">Sep 6<span>th</span>, 2012</time>
        
         | <a href="#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>Having learnt how to remove and add monitor&#8217;s, meta-data and data servers (mon&#8217;s, mds&#8217;s
and osd&#8217;s) for my small two node Ceph cluster. I want to say that it wasn&#8217;t too hard to
do, the ceph website does have documentation for this.</p>

<p>As the default CRUSH map replicates across OSD&#8217;s I wanted to try replicating data across
hosts just to see what would happen. In a real world scenario I would probably treat
individual hosts in a rack as a failure unit and if I had more than one rack of storage,
I would want to treat each rack as the minimum unit.</p>

<p>One of the coolest features of ceph is that it allows me to play with different mappings
and configurations of where my data gets allocated. There aren&#8217;t many (if any) storage
systems that I know of which provides this type of capability.</p>

<p>So the steps that I went through to get to what I wanted&#8230;</p>

<p>First I had to dump the CRUSH map from my cluster of two nodes and three (very unbalanced OSD&#8217;s so I can play with the weights).</p>

<pre><code>ceph osd getcrushmap -o /tmp/mycrushmap
</code></pre>

<p>The CRUSH map that is created is a binary file it must be decoded to plain text before
you can edit it.</p>

<pre><code>crushtool -d /tmp/mycrushmap &gt; /tmp/mycrushmap.txt
</code></pre>

<p>Here&#8217;s the map that is decoded from the binary file</p>

<pre><code># begin crush map

# devices
device 0 osd.0
device 1 osd.1
device 2 osd.2

# types
type 0 osd
type 1 host
type 2 rack
type 3 row
type 4 room
type 5 datacenter
type 6 pool

# buckets
host x.y.z.194 {
        id -2           # do not change unnecessarily
        # weight 2.000
        alg straw
        hash 0  # rjenkins1
        item osd.1 weight 1.000
        item osd.0 weight 1.000
}
host x.y.z.138 {
        id -4           # do not change unnecessarily
        # weight 1.000
        alg straw
        hash 0  # rjenkins1
        item osd.2 weight 1.000
}
rack rack-1 {
        id -3           # do not change unnecessarily
        # weight 3.000
        alg straw
        hash 0  # rjenkins1
        item x.y.z.194 weight 2.000
        item x.y.z.138 weight 1.000
}
pool default {
        id -1           # do not change unnecessarily
        # weight 2.000
        alg straw
        hash 0  # rjenkins1
        item rack-1 weight 2.000
}

# rules
rule data {
        ruleset 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step choose firstn 0 type osd
        step emit
}
rule metadata {
        ruleset 1
        type replicated
        min_size 1
        max_size 10
        step take default
        step choose firstn 0 type osd
        step emit
}
rule rbd {
        ruleset 2
        type replicated
        min_size 1
        max_size 10
        step take default
        step choose firstn 0 type osd
        step emit
}

# end crush map
</code></pre>

<p>The relevant chunks of the config that I&#8217;m interested in is the <strong>rule NAME {}</strong> blocks.
As I&#8217;m interested in making my data, meta-data and probably my rbd rule replicate across hosts, I naturally made the rule look like this</p>

<pre><code>rule data {
        ruleset 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step choose firstn 0 type host
        step emit
}
</code></pre>

<p>The above change is apparently incorrect as the last step before the <em>step emit</em> needs
to be a device of some sort. I had found this out after posting the ceph-devel mailing
list. There were two proposed solutions (thanks to Greg from inktank), the first
proposed rule was</p>

<pre><code>rule data {
        ruleset 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step choose firstn 0 type host
        step choose firstn 1 osd
        step emit
}
</code></pre>

<p>Which selects <em>n</em> hosts then the first osd from each host, but it can&#8217;t deal with an entire hosts failed OSD&#8217;s. The second proposed rule was</p>

<pre><code>rule data {
        ruleset 0
        type replicated
        min_size 1
        max_size 10
        step take default
        step chooseleaf firstn 0 type host
        step emit
}
</code></pre>

<p>The above rule will select <em>n</em> hosts and an OSD from the host. It&#8217;s pretty obvious that
the second rule is the one that I want. I would expect that if I had more machines in
racks and rows I could probably just replace host with rack, row or even data-center.</p>

<p>With the second proposed rule, I made the changes to <em>mycrushmap.txt</em>. Once the changes
are made, I had to compile the map into a binary format that the ceph cluster
understands, this can be done by</p>

<pre><code>crushtool -c /tmp/mycrushmap.txt -o /tmp/mycrushmap.new
</code></pre>

<p>Once the map is compiled it must then be applied to the cluster</p>

<pre><code>ceph osd setcrushmap -i /tmp/mycrushmap.new
</code></pre>

<p>The above is documented on the ceph website. Once I applied the new CRUSH map I ran a <em>ceph -w</em> to see that
the system had detected the changes and it then started to move data around on its own. I&#8217;ll need to play
with pulling out the network cable or SATA cables to see how the system behaves from me causing catastrophic
failures in the test system.</p>

<p>I&#8217;m pretty sure I took the long way around to making the changes, there must be a more dynamic way of
changing the system.</p>

<p>To recap and review the above operation, it&#8217;s again no harder than my reference system that I know, which is
GPFS. GPFS doesn&#8217;t allow me to do what ceph allows me to do. I would however like to see some more visible
documentation relating to the CRUSH configuration parameters and tuneables.</p>

<p>So far this has been a distraction from my main day job, but this will certainly help
with the project that I am working on in the long run.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Jimmy Tang</span></span>










  


<time datetime="2012-09-06T21:44:00+01:00" pubdate data-updated="true">Sep 6<span>th</span>, 2012</time>


<span class="categories">
  
    <a class='category' href='/blog/categories/ceph/'>ceph</a>, <a class='category' href='/blog/categories/linux/'>linux</a>, <a class='category' href='/blog/categories/storage/'>storage</a>
  
</span>



      

    </p>
    
      <div class="sharing">
  
    
      <a href="http://twitter.com/share" title="Tweet this" class="twitter-share" target="_blank">Tweet</a>
    
  
  
    
      <a title="+1 on Google Plus" class="googleplus-share" href="https://plusone.google.com/_/+1/confirm?hl=en&url=http://jcftang.github.com/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/" target="_blank">+1</a>
    
  
  
    
    <a title="Share on Facebook" class="facebook-share" href="http://www.facebook.com/sharer.php?u=http://jcftang.github.com/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/" target="_blank">Share</a>
    
  
  

</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2012/09/04/adding-an-osd-to-a-ceph-cluster/" title="Previous Post: Adding an OSD to a Ceph Cluster">&laquo; Adding an OSD to a Ceph Cluster</a>
      
      
        <a class="basic-alignment right" href="/2012/09/21/ceph-v0-dot-48-dot-2-argonaut-released/" title="Next Post: Ceph V0.48.2 ARGONAUT RELEASED">Ceph V0.48.2 ARGONAUT RELEASED &raquo;</a>
      
    </p>
  </footer>
</article>

<section>
  <h1>Comments</h1>
  

  
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  
</section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Related Posts</h1>
  <ul class="posts">
    
      <li class="related">
        <a href="/2012/09/04/adding-an-osd-to-a-ceph-cluster/">Adding an OSD to a Ceph Cluster</a>
      </li>
    
      <li class="related">
        <a href="/2012/09/23/a-poor-mans-nas-device-with-ceph/">A poor man's NAS device with Ceph</a>
      </li>
    
      <li class="related">
        <a href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/">Digital Preservation and Archiving is a HPC problem?</a>
      </li>
    
      <li class="related">
        <a href="/2012/07/05/thoughts-on-using-the-likes-of-tahoe-lafs-and-git-annex-as-a-backend-storage-system-for-digital-preservation/">Thoughts on using the likes of tahoe-lafs and git-annex as a backend storage system for Digital Preservation</a>
      </li>
    
      <li class="related">
        <a href="/2012/02/16/why-linux-isnt-the-only-platform-to-target-when-building-applications/">Why Linux isn't the only platform to target when building applications</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2012/11/04/there-is-light-on-the-otherside/">There is light on the otherside</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/27/crowbar-for-deploying-systems/">Crowbar for deploying systems</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/23/ceph-v0-dot-53-released/">Ceph V0.53 RELEASED</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/13/hydracamp-2012-penn-state/">Hydracamp 2012 - Penn State</a>
      </li>
    
      <li class="post">
        <a href="/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/">Digital Preservation and Archiving is a HPC problem?</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets" data-user="jcftang" data-count="4" data-replies="false">
    <li class="loading">Status updating...</li>
  </ul>
  
    <a href="//twitter.com/jcftang" class="twitter-follow-button" data-show-count="false">Follow @jcftang</a>
  
</section>







  <a href="https://plus.google.com/115375964029787125148?rel=author">
    <img src="https://ssl.gstatic.com/images/icons/gplus-32.png" alt="Google Plus icon">
  </a>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - Jimmy Tang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jcftang';
			var disqus_developer = '0';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://jcftang.github.com/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/';
        var disqus_url = 'http://jcftang.github.com/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>






</body>
</html>
