
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Adding an OSD to a Ceph Cluster - Jimmy Tang</title>
  <meta name="author" content="Jimmy Tang">
  <meta name="Generator" content="Jekyll & Octopress (http://octopress.org)">

  
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jcftang.github.com/2012/09/04/adding-an-osd-to-a-ceph-cluster/">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/octopress.min.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Jimmy Tang" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-8597777-2']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  

   
  <link href="/octopress-favicon.png" rel="icon">
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Jimmy Tang</a></h1>
  
    <h2>Yet more random mutterings</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jcftang.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/archives">Archives</a></li>
  <li><a href="/blog/categories">Categories</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>











<article class="hentry " role="article">
  
  <header>
    <h1 class="entry-title">

Adding an OSD to a Ceph Cluster

</h1>

    
      <p class="meta">
        








  


<time datetime="2012-09-04T18:42:00+08:00" pubdate data-updated="true">Sep 4<span>th</span>, 2012</time>
        
         | <a href="#disqus_thread">Comments</a>
        
         &bull; <a rel="bookmark" href="/2012/09/04/adding-an-osd-to-a-ceph-cluster/">Permalink</a>
      </p>
    
  </header>

<div class="entry-content"><p>Having created a small single node Ceph cluster with following the <a href="http://ceph.com/docs/master/start/quick-start/">5 minute quickstart</a> guide I was able to create a single node cluster with one OSD.</p>

<p>This probably wouldn&#39;t be the first post that someone has written about this topic.</p>

<p>I&#39;ve verified that it works in my test environment of Scientific Linux
6 by mounting the system with FUSE.</p>

<p>Here&#39;s my <em>fstab</em> to describe my disk layout</p>
<div class="highlight"><pre><code class="text"># /etc/fstab
# Created by anaconda on Fri Jul  6 14:27:56 2012
#
# Accessible filesystems, by reference, are maintained under &#39;/dev/disk&#39;
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
/dev/mapper/vg_ceres-lv_root /                     ext4    defaults,user_xattr        1 1
UUID=4eb5efad-dbcd-4a9f-8187-d8ffa913e147 /boot    ext4    defaults        1 2
/dev/mapper/vg_ceres-lv_home /data1                ext4 defaults,user_xattr        1 2
/dev/sdb /data                                     ext4 defaults,user_xattr        1 2
/dev/mapper/vg_ceres-lv_swap swap                  swap    defaults        0 0
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
</code></pre></div>
<p>Running a <em>df -h</em> gives this</p>
<div class="highlight"><pre><code class="text">$ df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg_ceres-lv_root
               50G  5.5G   42G  12% /
tmpfs                 938M     0  938M   0% /dev/shm
/dev/sda1             485M   80M  380M  18% /boot
/dev/sdb              230G  1.2G  217G   1% /data
ceph-fuse             230G   13G  217G   6% /mnt
/dev/mapper/vg_ceres-lv_home
              176G  188M  167G   1% /data1
</code></pre></div>
<p>I&#39;m using an old desktop machine so I can plonk some &ldquo;files&rdquo; on it so I
can dogfood the test system in administering the bits and pieces of Ceph.</p>

<p>Here&#39;s my current Ceph configuration (before I add a new OSD)</p>
<div class="highlight"><pre><code class="text">[global]
    #auth supported = cephx
    #keyring = /etc/ceph/ceph.keyring
    filestore xattr use omap = true

[osd]
    osd journal size = 1000
    filestore xattr use omap = true

[mon.a]
    host = x.y.z.194
    mon addr = x.y.z.194:6789
    mon data = /data/mon.$id
[mds.a]
    host = x.y.z.194
    mon data = /data/mds.$id

[osd.0]
    host = x.y.z.194
    osd data = /data/osd.$id
    osd journal = /data/osd.$id.journal
</code></pre></div>
<p>I was a little caught out by the <em>osd journal</em> setting, I did not realise
that I needed to set this value if I set a journal size.</p>

<p>So to add a new OSD to a running system I followed the instructions at
<a href="http://ceph.com/docs/master/ops/manage/grow/osd/">http://ceph.com/docs/master/ops/manage/grow/osd/</a>. This involves
allocating a new OSD id, editting the ceph.conf file, formatting the
OSD then adjusting the CRUSH map to allocate data to the new OSD.</p>
<div class="highlight"><pre><code class="text">ceph osd create
1
</code></pre></div>
<p>I then added these lines to my ceph.conf file (as this is my test system,
I&#39;ve ignored all sensible naming conventions)</p>
<div class="highlight"><pre><code class="text">[osd.1]
    host = x.y.z.194
    osd data = /data$id/osd.$id
    osd journal = /data$id/osd.$id.journal
</code></pre></div>
<p>I then create the directory for osd.1</p>
<div class="highlight"><pre><code class="text">mkdir /data1/osd.1
</code></pre></div>
<p>Once the above is done, I need to initialise the osd data directory,
this can be done with the following command.</p>
<div class="highlight"><pre><code class="text">ceph-osd -i 1 --mkfs
</code></pre></div>
<p>As I am not using any authentication (for now) I do not bother with keys
and the such.</p>

<p>With the above done, one can verify that the OSD has been added to the
system by executing the following command</p>
<div class="highlight"><pre><code class="text"># ceph osd tree
dumped osdmap tree epoch 10
# id    weight  type name       up/down reweight
-1      1       pool default
-3      1               rack unknownrack
-2      1                       host x.y.z.194
0       1                               osd.0   up      1

1       0       osd.1   down    0
</code></pre></div>
<p>Once the osd is in the cluster, it must be added to the CRUSH map. Given the above, the command that I need to execute would be</p>
<div class="highlight"><pre><code class="text">ceph osd crush set 1 osd.1 1.0 pool=default rack=unknownrack host=x.y.z.194
</code></pre></div>
<p>Running the osd tree command again will show that I have added the OSD to my host</p>
<div class="highlight"><pre><code class="text"># ceph osd tree
dumped osdmap tree epoch 11
# id    weight  type name       up/down reweight
-1      2       pool default
-3      2               rack unknownrack
-2      2                       host x.y.z.194
0       1                               osd.0   up      1
1       1                               osd.1   down    0
</code></pre></div>
<p>However the state is down for <em>osd.1</em>, it must be brought up before it
is usable. It can be brought up by doing,</p>
<div class="highlight"><pre><code class="text"># service ceph -a start osd
# ceph osd tree
dumped osdmap tree epoch 13
# id    weight  type name       up/down reweight
-1      2       pool default
-3      2               rack unknownrack
-2      2                       host x.y.z.194
0       1                               osd.0   up      1
1       1                               osd.1   up      1
</code></pre></div>
<p>If I do a <em>df -h</em> I should see an increase in the space that is available.</p>
<div class="highlight"><pre><code class="text"># df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg_ceres-lv_root
               50G  5.5G   42G  12% /
tmpfs                 938M     0  938M   0% /dev/shm
/dev/sda1             485M   80M  380M  18% /boot
/dev/sdb              230G  1.2G  217G   1% /data
ceph-fuse             405G   23G  382G   6% /mnt
/dev/mapper/vg_ceres-lv_home
                  176G  1.2G  166G   1% /data1
</code></pre></div>
<p>Given that I have in the past administered GPFS and Lustre filesystem
in production, this doesn&#39;t look too bad. I don&#39;t know the system that
well, but the configuration is pretty sensible and straight forward.</p>

<p>It&#39;s no harder than GPFS where one needs to create an NSD, then add the
NSD to a GPFS filesystem, nor is it more harder than just <em>mounting</em>
and OST in Lustre.</p>

<p>There does seem to be one or two additional things that a Ceph admin
would need to know before they can admin it optimally. However from the
initial playing around it looks like it needs more documentation, for
the seasoned parallel/distributed sysadmin this system is pretty neat
and tidy. However for the less experienced it looks like it might be a
bit hard to grasp some of the concepts before one can be an effective
admin of Ceph.</p>

<p>Now that I&#39;ve gone through the process of adding an OSD to my small test
cluster, I think the next thing to try is to play with the CRUSH map to
see if I can get Ceph replicate data between my OSD&#39;s even though
they are on the one machine. I guess the place to look at next is
<a href="http://ceph.com/wiki/Adjusting_replication_level">http://ceph.com/wiki/Adjusting_replication_level</a></p>

<p>Without getting into the finer details of all this, in GPFS you can only
have a replica size of 1 or 2 and you can only play with failure groups
of NSD&#39;s and nodes. GPFS does a lot to hide things from the admin, this
is probably a good thing. Lustre doesn&#39;t allow replicas at all (or
raid1). As powerful as Ceph is, I would imagine at some point someone
will ask &ldquo;can I just have a tool to set the replica count assuming I
have configured my machines and OSD lists accordingly&rdquo;</p>

<p>However the sysadmin in me just wants documentation as Ceph seems to
be almost ready for competing against GPFS and Lustre (as well as the
other parallel and distributed file systems).</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Jimmy Tang</span></span>










  


<time datetime="2012-09-04T18:42:00+08:00" pubdate data-updated="true">Sep 4<span>th</span>, 2012</time>


<span class="categories">
  
    <a class='category' href='/blog/categories/ceph/'>ceph</a>, <a class='category' href='/blog/categories/linux/'>linux</a>, <a class='category' href='/blog/categories/storage/'>storage</a>
  
</span>



      

    </p>
    
      <div class="sharing">
  
    
      <a href="http://twitter.com/share" title="Tweet this" class="twitter-share" target="_blank">Tweet</a>
    
  
  
    
      <a title="+1 on Google Plus" class="googleplus-share" href="https://plusone.google.com/_/+1/confirm?hl=en&url=http://jcftang.github.com/2012/09/04/adding-an-osd-to-a-ceph-cluster/" target="_blank">+1</a>
    
  
  
    
    <a title="Share on Facebook" class="facebook-share" href="http://www.facebook.com/sharer.php?u=http://jcftang.github.com/2012/09/04/adding-an-osd-to-a-ceph-cluster/" target="_blank">Share</a>
    
  
  

</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/2012/09/02/scientific-linux-6-build-environment-for-ceph/" title="Previous Post: Scientific Linux 6 build environment for Ceph">&laquo; Scientific Linux 6 build environment for Ceph</a>
      
      
        <a class="basic-alignment right" href="/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/" title="Next Post: Going from replicating across OSD's to replicating across hosts in a Ceph cluster">Going from replicating across OSD's to replicating across hosts in a Ceph cluster &raquo;</a>
      
    </p>
  </footer>
</article>

<section>
  <h1>Comments</h1>
  

  
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  
</section>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Related Posts</h1>
  <ul class="posts">
    
      <li class="related">
        <a href="/2012/09/06/going-from-replicating-across-osds-to-replicating-across-hosts-in-a-ceph-cluster/">Going from replicating across OSD's to replicating across hosts in a Ceph cluster</a>
      </li>
    
      <li class="related">
        <a href="/2012/02/12/cports-for-building-applications-and-libraries-for-hpc-systems/">cports for building applications and libraries for HPC systems</a>
      </li>
    
      <li class="related">
        <a href="/2012/07/05/thoughts-on-using-the-likes-of-tahoe-lafs-and-git-annex-as-a-backend-storage-system-for-digital-preservation/">Thoughts on using the likes of tahoe-lafs and git-annex as a backend storage system for Digital Preservation</a>
      </li>
    
      <li class="related">
        <a href="/2012/07/06/installing-ceph-on-sl6/">Installing Ceph on SL6</a>
      </li>
    
      <li class="related">
        <a href="/2012/09/23/a-poor-mans-nas-device-with-ceph/">A poor man's NAS device with Ceph</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2014/02/25/hydra-europe-2014/">Hydra Europe 2014</a>
      </li>
    
      <li class="post">
        <a href="/2013/12/31/building-a-private-cloud-with-saltstack-as-the-cloud-controller/">Building a private cloud with saltstack as the cloud controller</a>
      </li>
    
      <li class="post">
        <a href="/2013/12/01/post-sc13-thoughts/">Post SC13 Thoughts</a>
      </li>
    
      <li class="post">
        <a href="/2013/08/02/accelerating-development-and-deployments-with-ansible/">Accelerating Development and Deployments with Ansible</a>
      </li>
    
      <li class="post">
        <a href="/2013/06/07/hydra-and-ansible/">Hydra and Ansible</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Latest Tweets</h1>
  <a class="twitter-timeline" href="https://twitter.com/jcftang" data-widget-id="372440603998437376">Tweets by @jcftang</a>
  <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>







  <a href="https://plus.google.com/115375964029787125148?rel=author">
    <img src="https://ssl.gstatic.com/images/icons/gplus-32.png" alt="Google Plus icon">
  </a>



  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Jimmy Tang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'jcftang';
			var disqus_developer = '0';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://jcftang.github.com/2012/09/04/adding-an-osd-to-a-ceph-cluster/';
        var disqus_url = 'http://jcftang.github.com/2012/09/04/adding-an-osd-to-a-ceph-cluster/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>






</body>
</html>
