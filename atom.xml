<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Jimmy Tang]]></title>
  <link href="http://jcftang.github.com/atom.xml" rel="self"/>
  <link href="http://jcftang.github.com/"/>
  <updated>2014-09-24T17:56:15+01:00</updated>
  <id>http://jcftang.github.com/</id>
  <author>
    <name><![CDATA[Jimmy Tang]]></name>
    <email><![CDATA[jcftang@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      




<title type="html"><![CDATA[Developer happiness with waf]]></title>
<link href="http://jcftang.github.com/2014/09/24/developer-happiness-with-waf/"/>
<updated>2014-09-24T17:40:46+01:00</updated>
<id>http://jcftang.github.com/2014/09/24/developer-happiness-with-waf</id>


      <content type="html"><![CDATA[<p>Having recently changed jobs (after a very long stint at TCD - just
short of 10yrs!) I’ve moved to a small startup. I’ve been working
on a few small bits and pieces of code, infrastructure, etc&hellip; I
had to make some stuff work and make it work well. So I decided to
use autotools to configure and build the application that I’m working
on.</p>

<p>As great as it was on my nice fast intel based machine, it was dog
slow on my target platform, especially when I had to regenerate the
autoconf scripts.</p>

<p>To get to the point, I ended up spending a day or two checking out
alternatives (that wasn’t cmake) and came across
<a href="https://code.google.com/p/waf/">waf</a>, the documentation isn’t all
that great if you just look at the website, I had to spend a few
hours digging around to get what I wanted.</p>

<p>Once I ported my application over to using waf, the configure and
build process was an order of magnitude faster than autotools. This
made a huge difference on my target platform. I noticed that waf
ships with some experimental plugins like the daemon plugin which
runs a build when it notices that somefiles have changed.</p>

<p>This daemon plugin is <em>great</em> it gives waf similar behaviour to
what java developers have with junit and eclipse. I’m finding that
waf is making me happy when I develop and fix stuff in the application
that I’m working on.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Hashdist for repeatably and reliably building an environment across platforms]]></title>
<link href="http://jcftang.github.com/2014/06/22/hashdist-for-repeatably-building-an-environment/"/>
<updated>2014-06-22T14:52:42+01:00</updated>
<id>http://jcftang.github.com/2014/06/22/hashdist-for-repeatably-building-an-environment</id>
<category term="hpc" /><category term="linux" />

      <content type="html"><![CDATA[<p>I&#39;ve been happily hacking at some packages for
<a href="https://hashdist.github.com">hashdist</a>, it&#39;s pretty nice, there
other build systems out there for dealing with building applications
and libraries with different combinations of compilers and numerical
libraries. Out of the lot I think hashdist has been the most satisfying
to use so far. It&#39;s still missing some bits and pieces to allow users
to use different compilers for key components (or everything).</p>

<p>Without explaining too much, it&#39;s basically taking inputs which define
a package and then generating an output hash to store the output of the
build. This means that as you modify the the environment and rebuild, it
will install the updated packages into a different location. Traditionally
I would have just overwritten the old versions or install it into a named
directory, then setup a module file for it etc&hellip; with hashdist there
is less messing around. It doesn&#39;t fully solve the problem of having
multiple versions of the software available at any one point in time,
but it does let you have many versions installed and you can create
different inputs profiles to get access to the &#39;builds&#39;. This means you
can version control your environment, it also means you can roll back
to a previous environment if something is broken or wrong.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[When to automate deployments and when not to?]]></title>
<link href="http://jcftang.github.com/2014/04/21/when-to-automate-deployments-and-when-not-to/"/>
<updated>2014-04-21T15:17:23+01:00</updated>
<id>http://jcftang.github.com/2014/04/21/when-to-automate-deployments-and-when-not-to</id>
<category term="ansible" /><category term="dri" /><category term="hydra" />

      <content type="html"><![CDATA[<p>We recently had Hydra Camp in Dublin in Trinity College Dublin which
went pretty well. I even got to talk a little about what we&#39;re doing
with Shibboleth and how we&#39;re deploying our systems.</p>

<p>We&#39;re deploy with ansible either by someone running the playbook by hand
or via buildbot which pushes out a build when tests pass successfully
on the master branch.</p>

<p>Someone in the camp asked at what complexity should you begin to automate
at, which I thought was a strange question since at DRI/TCD we thought
of deploying as automatically as possible from day 1.</p>

<p>My response to the question of when to automate was to automate it if you
know that you need documentation or know that there might be a second
time in setting up the system. Which pretty meant that we automate as
much as we can.</p>

<p>Automating means you at least have a script as documentation and as people
leave/join the project you have a chance of figuring out whats going
on. This is especially useful when you have a CI or CD styled system.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Hydra Europe 2014 &rarr;]]></title>
<link href="https://wiki.duraspace.org/display/hydra/Hydra+Europe+2014"/>
<updated>2014-02-25T10:16:09+00:00</updated>
<id>http://jcftang.github.com/2014/02/25/hydra-europe-2014</id>
<category term="dri" /><category term="hydra" /><category term="preservation" />

      <content type="html"><![CDATA[<p>It&#39;s that time of the year again and it seems we&#39;re going to hold another
Hydra related event in work at Trinity College, Dublin.</p>

<p>For more information see
<a href="https://wiki.duraspace.org/display/hydra/Hydra+Europe+2014">https://wiki.duraspace.org/display/hydra/Hydra+Europe+2014</a></p>
<p><a rel="bookmark" href="http://jcftang.github.com/2014/02/25/hydra-europe-2014/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Building a private cloud with saltstack as the cloud controller]]></title>
<link href="http://jcftang.github.com/2013/12/31/building-a-private-cloud-with-saltstack-as-the-cloud-controller/"/>
<updated>2013-12-31T12:55:26+00:00</updated>
<id>http://jcftang.github.com/2013/12/31/building-a-private-cloud-with-saltstack-as-the-cloud-controller</id>


      <content type="html"><![CDATA[<p>After going to SC13 and being at a few BoF&#39;s and hearing
some people talk about their operations and potentially using
<a href="http://www.saltstack.com/">Salt</a> to replace the likes of puppet and chef,
I decided to learn a little more about Salt.</p>

<p>In particular since I have an old laptop lying around at
home, I decided to setup a little private cloud. I followed this <a href="http://www.saltstack.com/salt-blog/2013/11/19/cloud-making-doesnt-have-to-be-so-hard-salt-virt-tutorial">blog
post</a>.</p>

<p>It mostly worked apart from some buggy behaviour in the seeding
process. It was certainly lower overhead in setting a salt managed
&#39;cloud&#39; than setting up a full on <a href="http://opennebula.org/">Open Nebula</a>
instance or even openstack.</p>

<p>It&#39;s pretty neat that a provisioned VM reports into the salt master
and is ready to be provisioned pretty quickly when it boots up. I can
certainly see the attraction of using Salt to setup a private cloud with
Salt as the controller.</p>

<p>In short compared to <a href="http://www.ansibleworks.com/">Ansible</a> Salt is a
bit more heavy weight than Ansible. I haven&#39;t done any like with like
comparisons but I do feel that Salt seems like the better option for
maintaining long running systems and Ansible is great for rapid redeploys
of systems (from a clean state)</p>

<p>I&#39;ve uploaded the configs that I have been using at home to github, they
can be found <a href="https://github.com/jcftang/salt-virt">here</a>, there&#39;s even a
few packer templates to build base images for CentOS 6.5 and Ubuntu 12.04.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Post SC13 Thoughts]]></title>
<link href="http://jcftang.github.com/2013/12/01/post-sc13-thoughts/"/>
<updated>2013-12-01T09:04:25+00:00</updated>
<id>http://jcftang.github.com/2013/12/01/post-sc13-thoughts</id>


      <content type="html"><![CDATA[<p>I was recently at SC13 and attended a number of Python HPC tutorials,
Data Management and HPC systems engineering and administration BOF&#39;s.</p>

<p>This year&#39;s SC13 much calmer than previous years, but I did pick up a
few new tools during the conference. However I was pretty surprised that
there is a real lack of devops, sysadmins etc&hellip; in the this space.</p>

<p>I was also surprised at how things like salt and ansible are being
adopted in this space.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Accelerating Development and Deployments with Ansible]]></title>
<link href="http://jcftang.github.com/2013/08/01/accelerating-development-and-deployments-with-ansible/"/>
<updated>2013-08-01T21:38:54+01:00</updated>
<id>http://jcftang.github.com/2013/08/01/accelerating-development-and-deployments-with-ansible</id>


      <content type="html"><![CDATA[<p>It&#39;s probably no secret that we use Ansible at our work place for
the project that I am working on. So far we&#39;ve used it to deploy and
maintain state for our Open Nebula deployment, Jenkins CI system, Ceph&#39;s
radosgw and our digital repository.</p>

<p>In fact I currently have a Jenkins job which deploys our software stack
using Ansible to a test system in our Open Nebula cluster. This has been
hugely beneficial to myself so far to be able to teardown and bring up
systems quickly to make sure our application is well tested and debugged.</p>

<p>Without going into a huge amount of detail, we&#39;ve been able to deploy our
systems with relative ease and repeatability. I&#39;ve got the configurations
up at my github account for those that are interested.</p>

<p>The best thing about these deployments is that the initial prototyping
was done in a set of vagrant managed virtual machines, this allowed me
to rapidly bring up and teardown systems to ensure we have everything
automated smoothly. On the flipside, the systems that get developed for
production usage get backported to our vagrant setup. This means that
we&#39;re able to provide a development environment for each developer which
is identical or as close as possible to our production systems.</p>

<p>Having the capability to develop and test on a local machine which is
close to or identical to our production system has accelerated our bug
finding and development process.</p>

<p>I&#39;m not too sure what the team think, since we&#39;re moving quite fast and
Ansible has allowed us to do so due to its ease of use, well it has at
least let me do so anyway.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Hydra and Ansible]]></title>
<link href="http://jcftang.github.com/2013/06/06/hydra-and-ansible/"/>
<updated>2013-06-06T18:11:40+01:00</updated>
<id>http://jcftang.github.com/2013/06/06/hydra-and-ansible</id>
<category term="dri" /><category term="team" />

      <content type="html"><![CDATA[<p>The team that I am working with right are very much agile and we&#39;re
doing quite a bit of outside in development of the repository that we&#39;re
building. We&#39;re mostly adopting a behaviour driven development with a
touch of test driven development. As a result we&#39;re very much in favour
of testing things out as much as we can and using the same environments
to develop against. As previously mentioned before I had originally
been using puppet and vagrant to build up the development harness and
experiment with tools/services that we might want to use for our system.</p>

<p>At somepoint I came across <a href="http://www.ansible.cc">ansible</a> and not long
after discovering it, I migrated a large chunk of the development and
test systems to using ansible. I&#39;ve even cooked up one or two ansible
modules as a result.</p>

<p>As a result of adopting ansible for building up our test and development
infrastructure, I&#39;ve collected the relevant playbooks and roles that a
person might want for deploying all the bits and pieces needed to roll
out a hydra-head. See <a href="https://github.com/jcftang/ansible-hydra">https://github.com/jcftang/ansible-hydra</a>, I have
a set of roles and a few example playbooks on setting up at least</p>

<ul>
<li>Tomcat (from the base repositories of RHEL6/Centos6/ScientificLinux6)</li>
<li>Fedora-Commons (the same version as is in the hydra-jetty repo)</li>
<li>Apache SOLR (the same version as is in the hydra-jetty repo)</li>
<li>Ruby (via RVM) in a user directory</li>
<li>Passenger with the installed version of Ruby</li>
<li>MySQL (from the base repositories of RHEL6/Centos6/ScientificLinux6)</li>
<li>Redis</li>
</ul>

<p>The configurations aren&#39;t quite production ready yet as they do require
some more work in setting up Fedora-Commons and SOLR the way we want. The
configurations are however fairly realistic and are daily use
for doing test deployments of our hydra-head (RoR application) or
experimenting with additional tools, configurations and systems such as
<a href="https://github.com/jcftang/ansible-ceph">ceph</a> - we&#39;re using the radosgw
to provide a realistic and local S3 service.</p>

<p>So far the configurations need polishing off and another playbook needs
to be created for continuous deployment of our hydra-head.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[DRI Planet is somewhat back in action &rarr;]]></title>
<link href="http://www.tchpc.tcd.ie/dri-planet"/>
<updated>2013-05-19T08:51:15+01:00</updated>
<id>http://jcftang.github.com/2013/05/19/dri-planet-is-somewhat-back-in-action</id>
<category term="dri" />

      <content type="html"><![CDATA[<p>I&#39;ve been meaning to fix the RSS aggregator at
<a href="http://www.tchpc.tcd.ie/dri-planet">http://www.tchpc.tcd.ie/dri-planet</a> for a while now, it&#39;s fixed!</p>

<p>Note that it&#39;s just something that aggregates news that I think are useful
for work. The aggregated links may or may not be affiliated with my work.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/05/19/dri-planet-is-somewhat-back-in-action/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[ZFS on Linux is usable and safe &rarr;]]></title>
<link href="https://groups.google.com/a/zfsonlinux.org/forum/?fromgroups=#!topic/zfs-announce/ZXADhyOwFfA"/>
<updated>2013-03-31T11:05:56+01:00</updated>
<id>http://jcftang.github.com/2013/03/31/zfs-on-linux-is-usable-and-safe</id>
<category term="dri" /><category term="hpc" /><category term="preservation" /><category term="storage" />

      <content type="html"><![CDATA[<p>It seems that ZFS On Linux reached a significant milestone, that is the
software is stable to use on Linux. This is pretty useful and significant
for the preservation and archivists community as it provides a more
reliable platform to build on. The LLNL guys must really want to mitigate
against silent data failures in their systems (they&#39;re running Lustre
on top of ZFS).</p>

<p>If ZFS is trustable or not we will know over time. At least with
ZFS&#39;s data protection features we will see less issues with silent
data corruption.</p>

<p>If one could build glusterfs or better yet, Ceph on top of ZFS it would
be pretty desirable.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/03/31/zfs-on-linux-is-usable-and-safe/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Testing and developing rails applications in a near production like environment]]></title>
<link href="http://jcftang.github.com/2013/03/27/testing-and-developing-rails-applications-in-a-near-production-like-environment/"/>
<updated>2013-03-27T11:57:41+00:00</updated>
<id>http://jcftang.github.com/2013/03/27/testing-and-developing-rails-applications-in-a-near-production-like-environment</id>
<category term="dri" /><category term="linux" /><category term="team" />

      <content type="html"><![CDATA[<p>At work we like to do lots of testing and behaviour driven development
since we have a number of stakeholders and institutions all working on
the same application. To make sure everyone is getting what they want
we&#39;re using cucumber to write our specifications; that is we&#39;re primarily
doing outside in developement of our system.</p>

<p>As such we like to test things in a near production like
environment&hellip; Having chosen to use <a href="https://github.com/thoughtbot/capybara-webkit">capybara-webkit</a> to test
the interface (at a very functional and simplistic level) on our
workstations. I decided to test drive the vagrant enviroment that I had
been working on the past few months. Btw, this environment is bootstrapped
with ansible, I&#39;m a much happier person since I ditched puppet.</p>

<p>Annoyingly in RHEL6 and friends QT is a little bit behind the current
times, we needed at least QT4.7 to run the javascript tests (which rely
on webkit). The first an obvious thing was to uninstall qt from the base
system and then install qt47 from atrpms-testing.</p>

<p>Little did I know that <a href="https://bugzilla.redhat.com/show_bug.cgi?id=886996">tomcat6 depends</a> on redhat-lsb which in turn
depends on qt-x11 and so on&hellip; My plans on testing and developing our
application in a near production environment almost got shot.</p>

<p>In the end the solution was to test the java script components with either
selenium or better yet <a href="https://github.com/jonleighton/poltergeist">poltergeist</a> for capybara. We chose poltergeist
as it runs headless. This isn&#39;t really a problem of cucumber/capybara,
but rather a problem of RHEL6 and friends with old packages.</p>

<p>&hellip;after sometime I have a half-way there set of ansible playbooks to
get me to where I need to be in a virtual environment&hellip;</p>

<p>&hellip;less rant &hellip;</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Amazon Redshift - a curiousity to me?]]></title>
<link href="http://jcftang.github.com/2013/03/24/amazon-redshift-a-curiousity-to-me/"/>
<updated>2013-03-24T11:16:54+00:00</updated>
<id>http://jcftang.github.com/2013/03/24/amazon-redshift-a-curiousity-to-me</id>
<category term="hpc" /><category term="storage" />

      <content type="html"><![CDATA[<p>I&#39;ve been recently reading up on Amazon <a href="">Redshift</a>, at first I thought
it was a parallel/distributed datastore like HDFS. At a second look in
reality its more of a distributed relational database. This in itself
is pretty cool for scaling applications with large data sets that happen
to need to be in a database; which is quite a few things.</p>

<p>From the initial reading of the architecture and docs, it looks like
Amazon built a job queuing system around postgres to schedule queries
out to its nodes in the cluster. What&#39;s curious to me is how do they
deal with failed nodes in the system and how they provision the system,
there must be an ordered set of operations to do so.</p>

<p>Another oddity is the mesh network that the system has, I&#39;m under the
impression that there is only 1x10gb network connection on the machine. I
wonder if the mesh network is just a virtual mesh network or if it&#39;s
really a real physical one. If it&#39;s a real physical mesh network then
each machine would require more than one network interface. It would
also be a cabling nightmare to build such a device and if Amazon has
built a large scale mesh network of machines, that would be pretty cool.
I wonder how much of the Redshift user base are latency, bandwidth or
compute bound when looking at large datasets; or if its really just the
challenge of having one real big database that can be queried.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Building Kelvin Supercomputer (time lapse) &rarr;]]></title>
<link href="http://www.youtube.com/watch?v=F0zon45WR-U&feature=youtu.be"/>
<updated>2013-03-22T07:46:26+00:00</updated>
<id>http://jcftang.github.com/2013/03/22/building-kelvin-supercomputer-time-lapse</id>
<category term="hpc" /><category term="tcd" /><category term="team" />

      <content type="html"><![CDATA[<p>I was cleaning up some files and I found
this time lapse that I did when we were building
<a href="http://www.tchpc.tcd.ie/resources/clusters/kelvin">Kelvin</a>, it&#39;s a few
years old now. Even by current standards it&#39;s still pretty respectable.</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('F0zon45WR-U');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/F0zon45WR-U?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/F0zon45WR-U/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=F0zon45WR-U" id="F0zon45WR-U" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">Kelvin_eINIS_with_music_with_logos_trimmed-trifonic-hp</div>
</a>
<div class="video-info" >Time lapse of building Kelvin capability computer</div>
</div>

<p>We had to unpack and install all the infiniband cards ourselves, cabled,
racked, installed, configured and burnt it in for production usage. The
cluster has 100 nodes, each node has 2 sockets, each socket has 6 cores
and 24gb of ram.</p>

<p>The most interesting thing about this machine was that we got it with
QLogic infiniscale/infinipath HCA&#39;s and switching. It was one of the if
not the lowest latency networking that we could get at the time.</p>

<p>When we were configuring this system we had also redesigned the storage
system roughly around this period and went with a cluster to cluster
GPFS configuration.</p>

<p>Oh and we threw out the old cluster which was about 10racks of IBM e326
machines in the process.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/03/22/building-kelvin-supercomputer-time-lapse/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[HydraCamp 2013 - Trinity College Dublin &rarr;]]></title>
<link href="http://www.tchpc.tcd.ie/hydracamp2013"/>
<updated>2013-02-08T10:02:12+00:00</updated>
<id>http://jcftang.github.com/2013/02/08/hydracamp-2013-trinity-college--dublin</id>
<category term="dri" /><category term="preservation" /><category term="ruby" /><category term="tcd" />

      <content type="html"><![CDATA[<p>After a few months of budgeting, negotiations and finding venues we&#39;ve
at Trinity College Dublin have committed to hosting a HydraCamp for
europe! So I&#39;m shamelessly plugging it here.</p>

<p>Trinity College Dublin as a part of Digital Repository of Ireland will
be organising Hydra Camp in the week of April 8th till 12th, 2013. This
will be a week long training course, which will be aimed at developers
who are interested in the Hydra framework for developing repositories.</p>

<p>For those that don&#39;t know Hydra is a framework built upon various
Fedora-Commons, Apache SOLR, Ruby and Ruby on Rails technologies. At
DRI we&#39;re using it to build up our prototype repository.</p>

<p>It&#39;s possible to use the Hydra framework to build repositories for
archiving and preservation.</p>

<h3>Resources</h3>

<ul>
<li>HydraCamp 2013 - <a href="http://www.tchpc.tcd.ie/hydracamp2013">http://www.tchpc.tcd.ie/hydracamp2013</a></li>
<li>Digital Repository of Ireland - <a href="http://www.dri.ie/">http://www.dri.ie/</a></li>
<li>Project Hydra - <a href="http://projecthydra.org/">http://projecthydra.org/</a></li>
<li>Data Curation Experts - <a href="http://curationexperts.wordpress.com/">http://curationexperts.wordpress.com/</a></li>
</ul>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/02/08/hydracamp-2013-trinity-college--dublin/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Deploying our hydra-head with jruby]]></title>
<link href="http://jcftang.github.com/2013/01/22/deploying-our-hydra-head-with-jruby/"/>
<updated>2013-01-22T19:49:02+00:00</updated>
<id>http://jcftang.github.com/2013/01/22/deploying-our-hydra-head-with-jruby</id>
<category term="dri" /><category term="java" /><category term="linux" /><category term="preservation" /><category term="ruby" />

      <content type="html"><![CDATA[<p>We&#39;ve been ramping up our development work on the project that I have been on
in the last month or so. One of the issues that we&#39;ve come across is the not
so good XML validation and parsing libraries that are available in the ruby
world compared to the java world.</p>

<p>So as an exercise I decided to see if I could make our prototype work
with jruby with the view of doing a test deployment on tomcat or some
other application server. One of the motivating reason for doing this
is to get access to the java based XML libraries for validation and
processing. It also means that if I can deploy to the same Tomcat server
where I&#39;m running SOLR and Fedora-Commons then it means I&#39;m saving myself
a chunk of work setting up and maintaining <em>mod_passenger</em>.</p>

<p>In short what I ended up doing was make sqlite3 as being dependent on
the ruby platform and creating a block for jruby. So here&#39;s the relevant
snippet from my <em>Gemfile</em></p>

<figure class='code'><div class='highlight'><table><td class='line-numbers' aria-hidden='true'><pre><div data-line='1' class='line-number'></div><div data-line='2' class='line-number'></div><div data-line='3' class='line-number'></div><div data-line='4' class='line-number'></div><div data-line='5' class='line-number'></div><div data-line='6' class='line-number'></div><div data-line='7' class='line-number'></div><div data-line='8' class='line-number'></div><div data-line='9' class='line-number'></div><div data-line='10' class='line-number'></div><div data-line='11' class='line-number'></div><div data-line='12' class='line-number'></div><div data-line='13' class='line-number'></div><div data-line='14' class='line-number'></div><div data-line='15' class='line-number'></div><div data-line='16' class='line-number'></div><div data-line='17' class='line-number'></div><div data-line='18' class='line-number'></div></pre></td><td class='main  ruby'><pre><div class='line'><span class="n">gem</span> <span class="s1">&#39;sqlite3&#39;</span><span class="p">,</span> <span class="ss">:platforms</span> <span class="o">=&gt;</span> <span class="ss">:ruby</span>
</div><div class='line'> </div><div class='line'><span class="n">platforms</span> <span class="ss">:jruby</span> <span class="k">do</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jruby-openssl&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord-jdbcsqlite3-adapter&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jruby-lint&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;warbler&#39;</span>
</div><div class='line'> </div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;actionmailer&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;actionpack&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord-jdbc-adapter&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activeresource&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activesupport&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jdbc-mysql&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;rack&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;rake&#39;</span>
</div><div class='line'><span class="k">end</span>
</div></pre></td></tr></table></div></figure>

<p>What I found was that jruby behaves oddly when I&#39;m behind a proxy, it
seems to blindly take my system proxy settings on my mac, so I had to
work around it.</p>

<p>Nokogiri seems to be subtly different when deployed with jruby and thus
it breaks a bunch of things, which is funny as this was the main reason
to testing out jruby so that we can access the java based XML libraries.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Making the jump and migrating my archlinux machines to use btrfs]]></title>
<link href="http://jcftang.github.com/2013/01/02/making-the-jump-and-migrating-my-archlinux-machines-to-use-btrfs/"/>
<updated>2013-01-02T15:18:00+00:00</updated>
<id>http://jcftang.github.com/2013/01/02/making-the-jump-and-migrating-my-archlinux-machines-to-use-btrfs</id>
<category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>Having a few days of time off from work I&#39;ve committed to migrating my
archlinux based laptops to using btrfs. I&#39;ve to date been just using
ext4 and nilfs2 (on an SD card) on my eeepc and plain old ext4 on the
bigger laptop.</p>

<p>The main motivation was that the two devices were pretty outdated and
I felt lucky with doing a major upgrade (replaced sysvinit with systemd
as recommended by the archlinux people)</p>

<p>I didn&#39;t want to reinstall my machines so I took the route of converting
the existing ext4 partitions to btrfs. I left my <em>/boot</em> partition as
ext2 for safety. Prior to converting to btrfs I had upgraded the two
laptops to using <a href="https://wiki.archlinux.org/index.php/Grub2">grub2</a>
first. I then proceeded to boot up my laptops with the
archlinux installer image via a usb key. Then just followed the
<a href="https://wiki.archlinux.org/index.php/Btrfs">btrfs</a> documentation at
the archlinux wiki.</p>

<p>The process went more smoothly than I had anticipated, I didn&#39;t run into
any major stumbling blocks, that said I did have to free up some space
for the migration to occur.</p>

<p>Once the systems were back up and running I enabled compression on btrfs
and defragged the systems</p>
<div class="highlight"><pre><code class="text">find / -xdev -type f -print -exec btrfs filesystem defrag &#39;{}&#39; \;
</code></pre></div>
<p>This compression feature made a huge difference on the space limited
eeepc, the compression feature reclaimed about 10% of the space on the
system. It&#39;s a shame that the RHEL based distros aren&#39;t quite supporting
btrfs yet there are quite a few nice features there that are very
attractive for pro-consumers and the enterprise. RHEL7 will have btrfs,
hopefully there will be backports to RHEL6.</p>

<p>Performance wise I haven&#39;t used the system in anger yet, so time will
tell if I&#39;m happy with btrfs or not.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Supercomputing 2012 and the arms race to exascale]]></title>
<link href="http://jcftang.github.com/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/"/>
<updated>2012-11-16T06:30:00+00:00</updated>
<id>http://jcftang.github.com/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale</id>
<category term="hpc" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>It was clear at this year&#39;s supercomputing conference that there wasn&#39;t
as much excitement as previous years. There wasn&#39;t much surprise as
nothing too revolutionary and radical was announced. In the past when
Bluegene/L and P arrived after the earth simulator there was an arms race
to being number 1 in the top500 list.  Even things like GPGPU&#39;s aren&#39;t
as cool anymore, everyone is selling effectively the same systems when
it comes to clusters. Not everyone has the budget to procure a specialist
machine like a NEC vector machine, a CRAY, Bluegene/Q etc&hellip;</p>

<p>The arms race to the top is just completely crazy, the top 50 or so
machines are so powerful compared to what a typical university or small
research lab might have access is too is completely skewed. At our site
we&#39;re probably about 0.5% of the top machine. In the past we were about
1-2% typically of the top machine, and we were about 5yrs behind the
curve. The top 500 list should really be renamed to the top 50 list and
the green 500 list or the HPCC list should be used instead as a measure
of the top machines in the world. Do people really think that LINPACK
is a good measure of how powerful a machine is going to be?</p>

<p>What was interesting at this year&#39;s conference was some of the papers
and panels were focused on project management and data management of
scientific datasets. I noticed that when people are starting to worry
about meta-data standards like librarians, people are starting to think
about archiving the data, they probably haven&#39;t thought about the access
component of preservation which will be amusing when they do realise
it. There is also a disconnect from some of the papers which focused on
silent data corruption at the storage and network layers. The archiving
and preservation space for HPC will need to deal with bit-flips, silent
data corruption, malicious users and all that funk that is related to it.</p>

<p>After having a few chats with some vendors during the week, /me eyes
them vendors. There seems to be a bit of a lack of understanding that
archiving and preservation of data isn&#39;t just about bulk, cheap and
reliable storage. There are apects to it which require data processing and
analytics of the data. It seems somewhat pointless to archive data and not
index it, process it and deliver it to who needs it. At somepoint the data
will be touched for checksumming, surrogate generation, delivery to the
end user. There also seems to be a lack of a guarantee of data-integrity
at the filesystem level. It appears that this responsiblity is left to
the application developer.</p>

<p>Would storage vendors (filesystems, devices the whole stack!) please
consider providing end to end data integrity? If the guys at the top are
trying to reach exascale computing in the next 5 years then the storage
component needs to catch up. Other sectors would also benefit from the
end to end data integrity built into storage systems.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[There is light on the otherside]]></title>
<link href="http://jcftang.github.com/2012/11/04/there-is-light-on-the-otherside/"/>
<updated>2012-11-04T19:52:00+00:00</updated>
<id>http://jcftang.github.com/2012/11/04/there-is-light-on-the-otherside</id>
<category term="dri" /><category term="linux" /><category term="scm" /><category term="team" />

      <content type="html"><![CDATA[<p>Having spent the best part of my Sunday afternoon playing with ansible
just to learn and see what all the fuss is about, I was pleasantly
surprised with it.</p>

<p>I had installed <a href="https://github.com/ansible/ansible">ansible</a> on my OSX
laptop and <a href="https://github.com/ansible/vagrant-ansible">vagrant-ansible</a>
for my vagrant test environment.</p>

<p>The plan was to try and re-create my current ruby on rails development
and test virtual machine with vagrant. A secondary goal was to get it
to work with both Ubuntu Precise (LTS) and Scientific Linux 6.</p>

<p>My attempt at doing the above can be found at
<a href="https://github.com/jcftang/tchpc-vagrant/tree/ansible">tchpc-vagrant</a>
in the ansible branch. You will need ansible installed on your host
machine. You don&#39;t need much installed in the target machine as ansible
is designed to login and execute commands as required, this is quite
refreshing. Compared to puppet and chef, if I were to roll this out
into production my overhead will be pretty low. This low overhead is
something that I really like as I don&#39;t need to setup an infrastructure
just to run puppet.</p>

<p>In short I was able to learn how ansible is supposed to work (I think)
and build up enough configuration to start up a vagrant vm with what I
need for to do rails development in a matter of hours.</p>

<p>One thing that did occur to me was the lack of windows support, given
that ansible is designed to use ssh to carry out its activities, finding
stock windows machines which run ssh is pretty slim. This is one area
which puppet (perhaps chef too) is better at. It&#39;s also one feature that
I would like as the vagrant vm&#39;s that we&#39;re using in work might be given
to windows users for testing and evaluation.</p>

<p>Going forward I think ansible will certainly be in my toolkit. There really is
a light on the otherside for mangement, deployment and orchestration.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Crowbar for deploying systems]]></title>
<link href="http://jcftang.github.com/2012/10/27/crowbar-for-deploying-systems/"/>
<updated>2012-10-27T20:07:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/27/crowbar-for-deploying-systems</id>
<category term="linux" />

      <content type="html"><![CDATA[<p>I&#39;ve been eyeing <a href="https://github.com/dellcloudedge/crowbar">crowbar</a>
recently, it looks pretty useful and interesting for deploying servers
and applications. I haven&#39;t seen much if at all any documentation out
there which suggests that people in the digital preservation and archiving
fields are implementing systems at scale, I&#39;m under the impression that
most systems/sites are building systems up one piece at a time without
much automation.</p>

<p>It seems to use <a href="http://www.opscode.com/chef/">chef</a> in the backend for
all the automation. I&#39;ve been relearning <a href="http://puppetlabs.com/">puppet</a>
recently so that I can have reproducible environments with
<a href="http://vagrantup.com/">Vagrant</a>.</p>

<p>There might be an advantage to learn and port all the existing modules
that I have already created and configured to chef instead of puppet. If
I did move to a chef automation in my vagrant environments then a few
years from now when we go to full production we might be able to deploy
the whole system from bare metal relatively quickly and repeatably.</p>

<p>Automating the deployments will mean that we will have documentation
on the infrastructure itself. Either which way there is still a need
to automate the fedora-commons, SOLR, mysql and postgres deployments at
some point.</p>

<p>After all this thinking and pondering, I&#39;m still using puppet. There&#39;s
still the likes of ansible, cfengine, bcfg2 and juju. There is a never
ending supply of these tools.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Ceph V0.53 RELEASED &rarr;]]></title>
<link href="http://ceph.com/releases/v0-53-released/"/>
<updated>2012-10-23T15:08:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/23/ceph-v0-dot-53-released</id>
<category term="ceph" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>There&#39;s a new release of Ceph, I hope that they release a stable soon so we can
do further evaluations of the Ceph storage
system. A few of my work colleagues are going to the <a href="http://www.inktank.com/news-events/event/ceph-workshops-amsterdam/">Ceph
workshop</a>
next week.</p>

<p>I&#39;m wondering if anyone has taken the CRUSH algorithm and used it in other
domains.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2012/10/23/ceph-v0-dot-53-released/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
</feed>
