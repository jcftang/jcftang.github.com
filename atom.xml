<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Jimmy Tang]]></title>
  <link href="http://jcftang.github.com/atom.xml" rel="self"/>
  <link href="http://jcftang.github.com/"/>
  <updated>2013-12-14T10:31:19+00:00</updated>
  <id>http://jcftang.github.com/</id>
  <author>
    <name><![CDATA[Jimmy Tang]]></name>
    <email><![CDATA[jcftang@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      




<title type="html"><![CDATA[Post SC13 Thoughts]]></title>
<link href="http://jcftang.github.com/2013/12/01/post-sc13-thoughts/"/>
<updated>2013-12-01T09:04:25+00:00</updated>
<id>http://jcftang.github.com/2013/12/01/post-sc13-thoughts</id>


      <content type="html"><![CDATA[<p>I was recently at SC13 and attended a number of Python HPC tutorials,
Data Management and HPC systems engineering and administration BOF&#39;s.</p>

<p>This year&#39;s SC13 much calmer than previous years, but I did pick up a
few new tools during the conference. However I was pretty surprised that
there is a real lack of devops, sysadmins etc&hellip; in the this space.</p>

<p>I was also surprised at how things like salt and ansible are being
adopted in this space.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Accelerating Development and Deployments with Ansible]]></title>
<link href="http://jcftang.github.com/2013/08/01/accelerating-development-and-deployments-with-ansible/"/>
<updated>2013-08-01T21:38:54+01:00</updated>
<id>http://jcftang.github.com/2013/08/01/accelerating-development-and-deployments-with-ansible</id>


      <content type="html"><![CDATA[<p>It&#39;s probably no secret that we use Ansible at our work place for
the project that I am working on. So far we&#39;ve used it to deploy and
maintain state for our Open Nebula deployment, Jenkins CI system, Ceph&#39;s
radosgw and our digital repository.</p>

<p>In fact I currently have a Jenkins job which deploys our software stack
using Ansible to a test system in our Open Nebula cluster. This has been
hugely beneficial to myself so far to be able to teardown and bring up
systems quickly to make sure our application is well tested and debugged.</p>

<p>Without going into a huge amount of detail, we&#39;ve been able to deploy our
systems with relative ease and repeatability. I&#39;ve got the configurations
up at my github account for those that are interested.</p>

<p>The best thing about these deployments is that the initial prototyping
was done in a set of vagrant managed virtual machines, this allowed me
to rapidly bring up and teardown systems to ensure we have everything
automated smoothly. On the flipside, the systems that get developed for
production usage get backported to our vagrant setup. This means that
we&#39;re able to provide a development environment for each developer which
is identical or as close as possible to our production systems.</p>

<p>Having the capability to develop and test on a local machine which is
close to or identical to our production system has accelerated our bug
finding and development process.</p>

<p>I&#39;m not too sure what the team think, since we&#39;re moving quite fast and
Ansible has allowed us to do so due to its ease of use, well it has at
least let me do so anyway.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Hydra and Ansible]]></title>
<link href="http://jcftang.github.com/2013/06/06/hydra-and-ansible/"/>
<updated>2013-06-06T18:11:40+01:00</updated>
<id>http://jcftang.github.com/2013/06/06/hydra-and-ansible</id>
<category term="dri" /><category term="team" />

      <content type="html"><![CDATA[<p>The team that I am working with right are very much agile and we&#39;re
doing quite a bit of outside in development of the repository that we&#39;re
building. We&#39;re mostly adopting a behaviour driven development with a
touch of test driven development. As a result we&#39;re very much in favour
of testing things out as much as we can and using the same environments
to develop against. As previously mentioned before I had originally
been using puppet and vagrant to build up the development harness and
experiment with tools/services that we might want to use for our system.</p>

<p>At somepoint I came across <a href="http://www.ansible.cc">ansible</a> and not long
after discovering it, I migrated a large chunk of the development and
test systems to using ansible. I&#39;ve even cooked up one or two ansible
modules as a result.</p>

<p>As a result of adopting ansible for building up our test and development
infrastructure, I&#39;ve collected the relevant playbooks and roles that a
person might want for deploying all the bits and pieces needed to roll
out a hydra-head. See <a href="https://github.com/jcftang/ansible-hydra">https://github.com/jcftang/ansible-hydra</a>, I have
a set of roles and a few example playbooks on setting up at least</p>

<ul>
<li>Tomcat (from the base repositories of RHEL6/Centos6/ScientificLinux6)</li>
<li>Fedora-Commons (the same version as is in the hydra-jetty repo)</li>
<li>Apache SOLR (the same version as is in the hydra-jetty repo)</li>
<li>Ruby (via RVM) in a user directory</li>
<li>Passenger with the installed version of Ruby</li>
<li>MySQL (from the base repositories of RHEL6/Centos6/ScientificLinux6)</li>
<li>Redis</li>
</ul>

<p>The configurations aren&#39;t quite production ready yet as they do require
some more work in setting up Fedora-Commons and SOLR the way we want. The
configurations are however fairly realistic and are daily use
for doing test deployments of our hydra-head (RoR application) or
experimenting with additional tools, configurations and systems such as
<a href="https://github.com/jcftang/ansible-ceph">ceph</a> - we&#39;re using the radosgw
to provide a realistic and local S3 service.</p>

<p>So far the configurations need polishing off and another playbook needs
to be created for continuous deployment of our hydra-head.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[DRI Planet is somewhat back in action &rarr;]]></title>
<link href="http://www.tchpc.tcd.ie/dri-planet"/>
<updated>2013-05-19T08:51:15+01:00</updated>
<id>http://jcftang.github.com/2013/05/19/dri-planet-is-somewhat-back-in-action</id>
<category term="dri" />

      <content type="html"><![CDATA[<p>I&#39;ve been meaning to fix the RSS aggregator at
<a href="http://www.tchpc.tcd.ie/dri-planet">http://www.tchpc.tcd.ie/dri-planet</a> for a while now, it&#39;s fixed!</p>

<p>Note that it&#39;s just something that aggregates news that I think are useful
for work. The aggregated links may or may not be affiliated with my work.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/05/19/dri-planet-is-somewhat-back-in-action/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[ZFS on Linux is usable and safe &rarr;]]></title>
<link href="https://groups.google.com/a/zfsonlinux.org/forum/?fromgroups=#!topic/zfs-announce/ZXADhyOwFfA"/>
<updated>2013-03-31T11:05:56+01:00</updated>
<id>http://jcftang.github.com/2013/03/31/zfs-on-linux-is-usable-and-safe</id>
<category term="dri" /><category term="hpc" /><category term="preservation" /><category term="storage" />

      <content type="html"><![CDATA[<p>It seems that ZFS On Linux reached a significant milestone, that is the
software is stable to use on Linux. This is pretty useful and significant
for the preservation and archivists community as it provides a more
reliable platform to build on. The LLNL guys must really want to mitigate
against silent data failures in their systems (they&#39;re running Lustre
on top of ZFS).</p>

<p>If ZFS is trustable or not we will know over time. At least with
ZFS&#39;s data protection features we will see less issues with silent
data corruption.</p>

<p>If one could build glusterfs or better yet, Ceph on top of ZFS it would
be pretty desirable.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/03/31/zfs-on-linux-is-usable-and-safe/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Testing and developing rails applications in a near production like environment]]></title>
<link href="http://jcftang.github.com/2013/03/27/testing-and-developing-rails-applications-in-a-near-production-like-environment/"/>
<updated>2013-03-27T11:57:41+00:00</updated>
<id>http://jcftang.github.com/2013/03/27/testing-and-developing-rails-applications-in-a-near-production-like-environment</id>
<category term="dri" /><category term="linux" /><category term="team" />

      <content type="html"><![CDATA[<p>At work we like to do lots of testing and behaviour driven development
since we have a number of stakeholders and institutions all working on
the same application. To make sure everyone is getting what they want
we&#39;re using cucumber to write our specifications; that is we&#39;re primarily
doing outside in developement of our system.</p>

<p>As such we like to test things in a near production like
environment&hellip; Having chosen to use <a href="https://github.com/thoughtbot/capybara-webkit">capybara-webkit</a> to test
the interface (at a very functional and simplistic level) on our
workstations. I decided to test drive the vagrant enviroment that I had
been working on the past few months. Btw, this environment is bootstrapped
with ansible, I&#39;m a much happier person since I ditched puppet.</p>

<p>Annoyingly in RHEL6 and friends QT is a little bit behind the current
times, we needed at least QT4.7 to run the javascript tests (which rely
on webkit). The first an obvious thing was to uninstall qt from the base
system and then install qt47 from atrpms-testing.</p>

<p>Little did I know that <a href="https://bugzilla.redhat.com/show_bug.cgi?id=886996">tomcat6 depends</a> on redhat-lsb which in turn
depends on qt-x11 and so on&hellip; My plans on testing and developing our
application in a near production environment almost got shot.</p>

<p>In the end the solution was to test the java script components with either
selenium or better yet <a href="https://github.com/jonleighton/poltergeist">poltergeist</a> for capybara. We chose poltergeist
as it runs headless. This isn&#39;t really a problem of cucumber/capybara,
but rather a problem of RHEL6 and friends with old packages.</p>

<p>&hellip;after sometime I have a half-way there set of ansible playbooks to
get me to where I need to be in a virtual environment&hellip;</p>

<p>&hellip;less rant &hellip;</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Amazon Redshift - a curiousity to me?]]></title>
<link href="http://jcftang.github.com/2013/03/24/amazon-redshift-a-curiousity-to-me/"/>
<updated>2013-03-24T11:16:54+00:00</updated>
<id>http://jcftang.github.com/2013/03/24/amazon-redshift-a-curiousity-to-me</id>
<category term="hpc" /><category term="storage" />

      <content type="html"><![CDATA[<p>I&#39;ve been recently reading up on Amazon <a href="">Redshift</a>, at first I thought
it was a parallel/distributed datastore like HDFS. At a second look in
reality its more of a distributed relational database. This in itself
is pretty cool for scaling applications with large data sets that happen
to need to be in a database; which is quite a few things.</p>

<p>From the initial reading of the architecture and docs, it looks like
Amazon built a job queuing system around postgres to schedule queries
out to its nodes in the cluster. What&#39;s curious to me is how do they
deal with failed nodes in the system and how they provision the system,
there must be an ordered set of operations to do so.</p>

<p>Another oddity is the mesh network that the system has, I&#39;m under the
impression that there is only 1x10gb network connection on the machine. I
wonder if the mesh network is just a virtual mesh network or if it&#39;s
really a real physical one. If it&#39;s a real physical mesh network then
each machine would require more than one network interface. It would
also be a cabling nightmare to build such a device and if Amazon has
built a large scale mesh network of machines, that would be pretty cool.
I wonder how much of the Redshift user base are latency, bandwidth or
compute bound when looking at large datasets; or if its really just the
challenge of having one real big database that can be queried.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Building Kelvin Supercomputer (time lapse) &rarr;]]></title>
<link href="http://www.youtube.com/watch?v=F0zon45WR-U&feature=youtu.be"/>
<updated>2013-03-22T07:46:26+00:00</updated>
<id>http://jcftang.github.com/2013/03/22/building-kelvin-supercomputer-time-lapse</id>
<category term="hpc" /><category term="tcd" /><category term="team" />

      <content type="html"><![CDATA[<p>I was cleaning up some files and I found
this time lapse that I did when we were building
<a href="http://www.tchpc.tcd.ie/resources/clusters/kelvin">Kelvin</a>, it&#39;s a few
years old now. Even by current standards it&#39;s still pretty respectable.</p>

<div class="ratio-4-3 embed-video-container" onclick="var myAnchor = document.getElementById('F0zon45WR-U');var tmpDiv = document.createElement('div');tmpDiv.innerHTML = '&lt;iframe style=&quot;vertical-align:top;width:100%;height:100%;position:absolute;&quot; src=&quot;http://www.youtube.com/embed/F0zon45WR-U?autoplay=1&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;';myAnchor.parentNode.replaceChild(tmpDiv.firstChild, myAnchor);return false;" title="click here to play">
<a class="youtube-lazy-link" style="width:100%;height:100%;background:#000 url(http://i2.ytimg.com/vi/F0zon45WR-U/0.jpg) center center no-repeat;background-size:contain;position:absolute" href="http://www.youtube.com/watch?v=F0zon45WR-U" id="F0zon45WR-U" onclick="return false;">
<div class="youtube-lazy-link-div"></div>
<div class="youtube-lazy-link-info">Kelvin_eINIS_with_music_with_logos_trimmed-trifonic-hp</div>
</a>
<div class="video-info" >Time lapse of building Kelvin capability computer</div>
</div>

<p>We had to unpack and install all the infiniband cards ourselves, cabled,
racked, installed, configured and burnt it in for production usage. The
cluster has 100 nodes, each node has 2 sockets, each socket has 6 cores
and 24gb of ram.</p>

<p>The most interesting thing about this machine was that we got it with
QLogic infiniscale/infinipath HCA&#39;s and switching. It was one of the if
not the lowest latency networking that we could get at the time.</p>

<p>When we were configuring this system we had also redesigned the storage
system roughly around this period and went with a cluster to cluster
GPFS configuration.</p>

<p>Oh and we threw out the old cluster which was about 10racks of IBM e326
machines in the process.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/03/22/building-kelvin-supercomputer-time-lapse/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[HydraCamp 2013 - Trinity College Dublin &rarr;]]></title>
<link href="http://www.tchpc.tcd.ie/hydracamp2013"/>
<updated>2013-02-08T10:02:12+00:00</updated>
<id>http://jcftang.github.com/2013/02/08/hydracamp-2013-trinity-college--dublin</id>
<category term="dri" /><category term="preservation" /><category term="ruby" /><category term="tcd" />

      <content type="html"><![CDATA[<p>After a few months of budgeting, negotiations and finding venues we&#39;ve
at Trinity College Dublin have committed to hosting a HydraCamp for
europe! So I&#39;m shamelessly plugging it here.</p>

<p>Trinity College Dublin as a part of Digital Repository of Ireland will
be organising Hydra Camp in the week of April 8th till 12th, 2013. This
will be a week long training course, which will be aimed at developers
who are interested in the Hydra framework for developing repositories.</p>

<p>For those that don&#39;t know Hydra is a framework built upon various
Fedora-Commons, Apache SOLR, Ruby and Ruby on Rails technologies. At
DRI we&#39;re using it to build up our prototype repository.</p>

<p>It&#39;s possible to use the Hydra framework to build repositories for
archiving and preservation.</p>

<h3>Resources</h3>

<ul>
<li>HydraCamp 2013 - <a href="http://www.tchpc.tcd.ie/hydracamp2013">http://www.tchpc.tcd.ie/hydracamp2013</a></li>
<li>Digital Repository of Ireland - <a href="http://www.dri.ie/">http://www.dri.ie/</a></li>
<li>Project Hydra - <a href="http://projecthydra.org/">http://projecthydra.org/</a></li>
<li>Data Curation Experts - <a href="http://curationexperts.wordpress.com/">http://curationexperts.wordpress.com/</a></li>
</ul>
<p><a rel="bookmark" href="http://jcftang.github.com/2013/02/08/hydracamp-2013-trinity-college--dublin/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Deploying our hydra-head with jruby]]></title>
<link href="http://jcftang.github.com/2013/01/22/deploying-our-hydra-head-with-jruby/"/>
<updated>2013-01-22T19:49:02+00:00</updated>
<id>http://jcftang.github.com/2013/01/22/deploying-our-hydra-head-with-jruby</id>
<category term="dri" /><category term="java" /><category term="linux" /><category term="preservation" /><category term="ruby" />

      <content type="html"><![CDATA[<p>We&#39;ve been ramping up our development work on the project that I have been on
in the last month or so. One of the issues that we&#39;ve come across is the not
so good XML validation and parsing libraries that are available in the ruby
world compared to the java world.</p>

<p>So as an exercise I decided to see if I could make our prototype work
with jruby with the view of doing a test deployment on tomcat or some
other application server. One of the motivating reason for doing this
is to get access to the java based XML libraries for validation and
processing. It also means that if I can deploy to the same Tomcat server
where I&#39;m running SOLR and Fedora-Commons then it means I&#39;m saving myself
a chunk of work setting up and maintaining <em>mod_passenger</em>.</p>

<p>In short what I ended up doing was make sqlite3 as being dependent on
the ruby platform and creating a block for jruby. So here&#39;s the relevant
snippet from my <em>Gemfile</em></p>

<figure class='code'><div class='highlight'><table><td class='line-numbers' aria-hidden='true'><pre><div data-line='1' class='line-number'></div><div data-line='2' class='line-number'></div><div data-line='3' class='line-number'></div><div data-line='4' class='line-number'></div><div data-line='5' class='line-number'></div><div data-line='6' class='line-number'></div><div data-line='7' class='line-number'></div><div data-line='8' class='line-number'></div><div data-line='9' class='line-number'></div><div data-line='10' class='line-number'></div><div data-line='11' class='line-number'></div><div data-line='12' class='line-number'></div><div data-line='13' class='line-number'></div><div data-line='14' class='line-number'></div><div data-line='15' class='line-number'></div><div data-line='16' class='line-number'></div><div data-line='17' class='line-number'></div><div data-line='18' class='line-number'></div></pre></td><td class='main  ruby'><pre><div class='line'><span class="n">gem</span> <span class="s1">&#39;sqlite3&#39;</span><span class="p">,</span> <span class="ss">:platforms</span> <span class="o">=&gt;</span> <span class="ss">:ruby</span>
</div><div class='line'> </div><div class='line'><span class="n">platforms</span> <span class="ss">:jruby</span> <span class="k">do</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jruby-openssl&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord-jdbcsqlite3-adapter&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jruby-lint&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;warbler&#39;</span>
</div><div class='line'> </div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;actionmailer&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;actionpack&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activerecord-jdbc-adapter&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activeresource&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;activesupport&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;jdbc-mysql&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;rack&#39;</span>
</div><div class='line'>  <span class="n">gem</span> <span class="s1">&#39;rake&#39;</span>
</div><div class='line'><span class="k">end</span>
</div></pre></td></tr></table></div></figure>

<p>What I found was that jruby behaves oddly when I&#39;m behind a proxy, it
seems to blindly take my system proxy settings on my mac, so I had to
work around it.</p>

<p>Nokogiri seems to be subtly different when deployed with jruby and thus
it breaks a bunch of things, which is funny as this was the main reason
to testing out jruby so that we can access the java based XML libraries.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Making the jump and migrating my archlinux machines to use btrfs]]></title>
<link href="http://jcftang.github.com/2013/01/02/making-the-jump-and-migrating-my-archlinux-machines-to-use-btrfs/"/>
<updated>2013-01-02T15:18:00+00:00</updated>
<id>http://jcftang.github.com/2013/01/02/making-the-jump-and-migrating-my-archlinux-machines-to-use-btrfs</id>
<category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>Having a few days of time off from work I&#39;ve committed to migrating my
archlinux based laptops to using btrfs. I&#39;ve to date been just using
ext4 and nilfs2 (on an SD card) on my eeepc and plain old ext4 on the
bigger laptop.</p>

<p>The main motivation was that the two devices were pretty outdated and
I felt lucky with doing a major upgrade (replaced sysvinit with systemd
as recommended by the archlinux people)</p>

<p>I didn&#39;t want to reinstall my machines so I took the route of converting
the existing ext4 partitions to btrfs. I left my <em>/boot</em> partition as
ext2 for safety. Prior to converting to btrfs I had upgraded the two
laptops to using <a href="https://wiki.archlinux.org/index.php/Grub2">grub2</a>
first. I then proceeded to boot up my laptops with the
archlinux installer image via a usb key. Then just followed the
<a href="https://wiki.archlinux.org/index.php/Btrfs">btrfs</a> documentation at
the archlinux wiki.</p>

<p>The process went more smoothly than I had anticipated, I didn&#39;t run into
any major stumbling blocks, that said I did have to free up some space
for the migration to occur.</p>

<p>Once the systems were back up and running I enabled compression on btrfs
and defragged the systems</p>
<div class="highlight"><pre><code class="text">find / -xdev -type f -print -exec btrfs filesystem defrag &#39;{}&#39; \;
</code></pre></div>
<p>This compression feature made a huge difference on the space limited
eeepc, the compression feature reclaimed about 10% of the space on the
system. It&#39;s a shame that the RHEL based distros aren&#39;t quite supporting
btrfs yet there are quite a few nice features there that are very
attractive for pro-consumers and the enterprise. RHEL7 will have btrfs,
hopefully there will be backports to RHEL6.</p>

<p>Performance wise I haven&#39;t used the system in anger yet, so time will
tell if I&#39;m happy with btrfs or not.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Supercomputing 2012 and the arms race to exascale]]></title>
<link href="http://jcftang.github.com/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale/"/>
<updated>2012-11-16T06:30:00+00:00</updated>
<id>http://jcftang.github.com/2012/11/16/supercomputing-2012-and-the-arms-race-to-exascale</id>
<category term="hpc" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>It was clear at this year&#39;s supercomputing conference that there wasn&#39;t
as much excitement as previous years. There wasn&#39;t much surprise as
nothing too revolutionary and radical was announced. In the past when
Bluegene/L and P arrived after the earth simulator there was an arms race
to being number 1 in the top500 list.  Even things like GPGPU&#39;s aren&#39;t
as cool anymore, everyone is selling effectively the same systems when
it comes to clusters. Not everyone has the budget to procure a specialist
machine like a NEC vector machine, a CRAY, Bluegene/Q etc&hellip;</p>

<p>The arms race to the top is just completely crazy, the top 50 or so
machines are so powerful compared to what a typical university or small
research lab might have access is too is completely skewed. At our site
we&#39;re probably about 0.5% of the top machine. In the past we were about
1-2% typically of the top machine, and we were about 5yrs behind the
curve. The top 500 list should really be renamed to the top 50 list and
the green 500 list or the HPCC list should be used instead as a measure
of the top machines in the world. Do people really think that LINPACK
is a good measure of how powerful a machine is going to be?</p>

<p>What was interesting at this year&#39;s conference was some of the papers
and panels were focused on project management and data management of
scientific datasets. I noticed that when people are starting to worry
about meta-data standards like librarians, people are starting to think
about archiving the data, they probably haven&#39;t thought about the access
component of preservation which will be amusing when they do realise
it. There is also a disconnect from some of the papers which focused on
silent data corruption at the storage and network layers. The archiving
and preservation space for HPC will need to deal with bit-flips, silent
data corruption, malicious users and all that funk that is related to it.</p>

<p>After having a few chats with some vendors during the week, /me eyes
them vendors. There seems to be a bit of a lack of understanding that
archiving and preservation of data isn&#39;t just about bulk, cheap and
reliable storage. There are apects to it which require data processing and
analytics of the data. It seems somewhat pointless to archive data and not
index it, process it and deliver it to who needs it. At somepoint the data
will be touched for checksumming, surrogate generation, delivery to the
end user. There also seems to be a lack of a guarantee of data-integrity
at the filesystem level. It appears that this responsiblity is left to
the application developer.</p>

<p>Would storage vendors (filesystems, devices the whole stack!) please
consider providing end to end data integrity? If the guys at the top are
trying to reach exascale computing in the next 5 years then the storage
component needs to catch up. Other sectors would also benefit from the
end to end data integrity built into storage systems.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[There is light on the otherside]]></title>
<link href="http://jcftang.github.com/2012/11/04/there-is-light-on-the-otherside/"/>
<updated>2012-11-04T19:52:00+00:00</updated>
<id>http://jcftang.github.com/2012/11/04/there-is-light-on-the-otherside</id>
<category term="dri" /><category term="linux" /><category term="scm" /><category term="team" />

      <content type="html"><![CDATA[<p>Having spent the best part of my Sunday afternoon playing with ansible
just to learn and see what all the fuss is about, I was pleasantly
surprised with it.</p>

<p>I had installed <a href="https://github.com/ansible/ansible">ansible</a> on my OSX
laptop and <a href="https://github.com/ansible/vagrant-ansible">vagrant-ansible</a>
for my vagrant test environment.</p>

<p>The plan was to try and re-create my current ruby on rails development
and test virtual machine with vagrant. A secondary goal was to get it
to work with both Ubuntu Precise (LTS) and Scientific Linux 6.</p>

<p>My attempt at doing the above can be found at
<a href="https://github.com/jcftang/tchpc-vagrant/tree/ansible">tchpc-vagrant</a>
in the ansible branch. You will need ansible installed on your host
machine. You don&#39;t need much installed in the target machine as ansible
is designed to login and execute commands as required, this is quite
refreshing. Compared to puppet and chef, if I were to roll this out
into production my overhead will be pretty low. This low overhead is
something that I really like as I don&#39;t need to setup an infrastructure
just to run puppet.</p>

<p>In short I was able to learn how ansible is supposed to work (I think)
and build up enough configuration to start up a vagrant vm with what I
need for to do rails development in a matter of hours.</p>

<p>One thing that did occur to me was the lack of windows support, given
that ansible is designed to use ssh to carry out its activities, finding
stock windows machines which run ssh is pretty slim. This is one area
which puppet (perhaps chef too) is better at. It&#39;s also one feature that
I would like as the vagrant vm&#39;s that we&#39;re using in work might be given
to windows users for testing and evaluation.</p>

<p>Going forward I think ansible will certainly be in my toolkit. There really is
a light on the otherside for mangement, deployment and orchestration.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Crowbar for deploying systems]]></title>
<link href="http://jcftang.github.com/2012/10/27/crowbar-for-deploying-systems/"/>
<updated>2012-10-27T20:07:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/27/crowbar-for-deploying-systems</id>
<category term="linux" />

      <content type="html"><![CDATA[<p>I&#39;ve been eyeing <a href="https://github.com/dellcloudedge/crowbar">crowbar</a>
recently, it looks pretty useful and interesting for deploying servers
and applications. I haven&#39;t seen much if at all any documentation out
there which suggests that people in the digital preservation and archiving
fields are implementing systems at scale, I&#39;m under the impression that
most systems/sites are building systems up one piece at a time without
much automation.</p>

<p>It seems to use <a href="http://www.opscode.com/chef/">chef</a> in the backend for
all the automation. I&#39;ve been relearning <a href="http://puppetlabs.com/">puppet</a>
recently so that I can have reproducible environments with
<a href="http://vagrantup.com/">Vagrant</a>.</p>

<p>There might be an advantage to learn and port all the existing modules
that I have already created and configured to chef instead of puppet. If
I did move to a chef automation in my vagrant environments then a few
years from now when we go to full production we might be able to deploy
the whole system from bare metal relatively quickly and repeatably.</p>

<p>Automating the deployments will mean that we will have documentation
on the infrastructure itself. Either which way there is still a need
to automate the fedora-commons, SOLR, mysql and postgres deployments at
some point.</p>

<p>After all this thinking and pondering, I&#39;m still using puppet. There&#39;s
still the likes of ansible, cfengine, bcfg2 and juju. There is a never
ending supply of these tools.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Ceph V0.53 RELEASED &rarr;]]></title>
<link href="http://ceph.com/releases/v0-53-released/"/>
<updated>2012-10-23T15:08:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/23/ceph-v0-dot-53-released</id>
<category term="ceph" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>There&#39;s a new release of Ceph, I hope that they release a stable soon so we can
do further evaluations of the Ceph storage
system. A few of my work colleagues are going to the <a href="http://www.inktank.com/news-events/event/ceph-workshops-amsterdam/">Ceph
workshop</a>
next week.</p>

<p>I&#39;m wondering if anyone has taken the CRUSH algorithm and used it in other
domains.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2012/10/23/ceph-v0-dot-53-released/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Hydracamp 2012 - Penn State &rarr;]]></title>
<link href="http://yourmediashelf.com/hydracamp/"/>
<updated>2012-10-13T07:34:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/13/hydracamp-2012-penn-state</id>
<category term="dri" /><category term="preservation" /><category term="ruby" />

      <content type="html"><![CDATA[<p>What do you do when you need a crash course on RoR, Hydra and frameworks for
digital preservation and archiving? You go to Hydracamp!</p>

<p>The syllabus was</p>

<ul>
<li>Day 1 - Rails, CRUD, TDD and Git</li>
<li>Day 2 - Collaborative development with Stories, Tickets, TDD and Git</li>
<li>Day 3 - Hydra, Fedora, XML and RDF (ActiveFedora and OM)</li>
<li>Day 4 - SOLR and Blacklight</li>
<li>Day 5 - Hydra-head, Hydra Access Controls</li>
</ul>

<p>Most of the training sessions were hands on from day 1 which was
refreshing, as it was hands on I getting the most out of the training
session. It would have been better if I had known more ruby to move
along some of the exercises more effectively.</p>

<p>To give an overview of what we had done (between ~30 people), we created
a ruby on rails application titled &ldquo;Twitter for Zombies&rdquo;. With this small
application everybody was frantically committing, pulling, merging and
pushing code. It was highly informative and a good learning experience
to see how fast things could move.</p>

<p>The training session also included a crash course into what Fedora and
SOLR does and how Hydra interacts with these components. The third and
fourth days were the most interesting as it showed how someone might
convert from a typical RoR application into an application which uses
Fedora as the persistance layer. The last day was really just a wrap up
and Q&amp;A session.</p>

<p>You could take a look at the <a href="https://github.com/projecthydra">github</a>
account for Project Hydra and have a peek at the hydracamp repo.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2012/10/13/hydracamp-2012-penn-state/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Digital Preservation and Archiving is a HPC problem?]]></title>
<link href="http://jcftang.github.com/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem/"/>
<updated>2012-10-07T13:42:00+01:00</updated>
<id>http://jcftang.github.com/2012/10/07/digital-preservation-and-archiving-is-a-hpc-problem</id>
<category term="hpc" /><category term="storage" />

      <content type="html"><![CDATA[<p>I shall be going to SC2012 next month, I plan on hitting a few of the
storage vendors for possible collaborations and flagging to them that
we&#39;re on the look out for storage systems. One of the first
observation that the reader will note is &ldquo;where is that link between
HPC and Digital Preservation and Archiving&rdquo;. It&#39;s probably not obvious
to most people, one of the big problems in the area of preservation
and archiving is the the amount of data involved and the varied types
of data. This is not taking into account of the issues with data
access patterns.</p>

<p>Given that a preservation and archiving project will want to provide a
trusted system, the system will want to read out every single byte
that was put in to verify that the data is correct at somepoint
(usually with some form of hashing).</p>

<p>Reading data out and checking that it&#39;s correct serially probably
isn&#39;t the smartest solution. Nor is copying the data into 2-3
locations (where each site is maintaining 2-3 copies for backups and
redundancy). The current and seemingly most popular solutions is to
dump the data to a few offsite locations (such as S3 or SWIFT)
compatible storage systems, then just hoping for the best that if
anyone of the sites is down or corrupted there site can be restored
from the other sites or from a backup. I need to delve deeper into the
storage and data-distribution strategies that some of the bigger
projects are taking. There has to be a smarter way of storing and
preserving data without having to make copies of things.</p>

<p>I&#39;ve often wondered how projects manage to copy/move data across
storage providers in a reasonable amount of time without needing to
wheel a few racks of disks around. It would also be interesting to see
the error rates of these systems and how often errors are
corrected. If they are corrected what is the computational cost of
doing this.</p>

<p>If you have a multi-terabyte archive the problem isn&#39;t too bad, the
more typical case these days might be in the order of the low hundreds
of terabytes. I could only imagine what lager scale sites must deal
with. I&#39;m still not a fan of moving a problem from a local site to a
remote site as it often shows that there is a lack of understanding to
the problem. Storage in the preservation and archiving domain will
probably turn into an IO and compute intensive operation at some
point, especially if you want to do something with the data.</p>
]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[SLURM-Bank that big script for banking in SLURM &rarr;]]></title>
<link href="https://github.com/jcftang/slurm-bank"/>
<updated>2012-09-29T19:52:00+01:00</updated>
<id>http://jcftang.github.com/2012/09/29/slurm-bank-that-big-script-for-banking-in-slurm</id>
<category term="hpc" /><category term="linux" />

      <content type="html"><![CDATA[<p>A co-worker of mine (Paddy Doyle) had originally hacked at a perl script
for reporting balances from SLURM&#39;s accounting system a year or two ago
and he had figured out that it might be possible to do some minimalistic
&#39;configuration&#39; and scripting to get a system that&#39;s very basic but
functional.</p>

<p>It was just one of those things that funding agencies wanted to justify
how the system was being used, GOLD was clunky and obtrusive and
complicated for what we wanted. Most of all we liked SLURM but not GOLD
and Maui which was needed to get full accounting and banking (most of
the features weren&#39;t used).</p>

<p>Being good and lazy engineers we got excited with the prospect of having
the option of replacing SLURM, Maui and GOLD with just plain old SLURM
we set out to write down the workflows for what we wanted to do and what
the user and funding agencies actually wanted. With those ideas in mind
we set out to implement as much as we could and needed in just plain
old sh/bash scripting with a splash of perl. Replacing two components
with one meant that we would have less work to do in the long run ;)</p>

<p>After a whole year of running with these scripts and just putting it
online, I&#39;ve noticed that there may be a few sites out there that might
be using our scripts and workflows. It would be nice to find out how
many people are using our implementation of a banking system in SLURM
and if it&#39;s driven by sysadmins looking to account for usage or is it
funding agencies looking for justification of the usage of a system.</p>

<p>I was going to be at the SLURM User Group Meeting 2012 to give a
short talk on our experiences with the SLURM-Bank scripts and workflow,
but sadly I have to be in the US during this meeting and my colleague
&ldquo;Paddy Doyle&rdquo; will there instead of me.  I would have liked to go and chat
with the developers of SLURM to push for more advanced banking/accounting
facilities in SLURM itself. Visiting BSC again would have been fun.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2012/09/29/slurm-bank-that-big-script-for-banking-in-slurm/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[Ceph V0.52 RELEASED &rarr;]]></title>
<link href="http://ceph.com/releases/v0-52-released/"/>
<updated>2012-09-28T15:52:00+01:00</updated>
<id>http://jcftang.github.com/2012/09/28/ceph-v0-dot-52-released</id>
<category term="ceph" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>The latest development branch of Ceph is out with some rather nice
looking features, what&#39;s probably the most useful are the RPM builds
for those that run RHEL6 like systems.</p>

<p>Still no real sight of backported kernel modules :P Also some of the
guys in work here just deployed a ~200tb Ceph installation which I&#39;ve
access to a 10tb RBD for doing backups on.</p>
<p><a rel="bookmark" href="http://jcftang.github.com/2012/09/28/ceph-v0-dot-52-released/">&#9875; Permalink</a></p>]]></content>
    </entry>
  
    <entry>
      




<title type="html"><![CDATA[A poor man's NAS device with Ceph]]></title>
<link href="http://jcftang.github.com/2012/09/23/a-poor-mans-nas-device-with-ceph/"/>
<updated>2012-09-23T08:59:00+01:00</updated>
<id>http://jcftang.github.com/2012/09/23/a-poor-mans-nas-device-with-ceph</id>
<category term="ceph" /><category term="linux" /><category term="storage" />

      <content type="html"><![CDATA[<p>Given that I have a number of old 64bit capable desktop machines and a
collection of hard drives at home, I could have run
<a href="https://tahoe-lafs.org/trac/tahoe-lafs">Tahoe-LAFS</a> like I do in work
for backup purposes. In fact Tahoe works quite well for the
technically capable user.</p>

<p>Recently I&#39;ve decided that I need a more central location at home to
store my photo collection (I love to take photos with my Canon DSLR
and Panasonic LX5). Traditionally I would have just fired up
<a href="http://git-annex.branchable.com/">git-annex</a> to track the data and
then setup a number of remotes to store the data, where one of them
might be Tahoe-LAFS and the rest might be portable hard drives, remote
machines etc&hellip;</p>

<p>I could have gone with any number of distributed storage solutions
such as <a href="http://www.gluster.org/">GlusterFS</a>,
<a href="http://www.irods.org">iRODS</a>,
<a href="http://xrootd.slac.stanford.edu/">xrootd</a>,
<a href="http://wiki.lustre.org/index.php/Main_Page">Lustre</a> or
<a href="http://www.xtreemfs.org/">xtreemfs</a>. I&#39;ve worked with some of these
systems in production and toyed with others. Since this is for a home
system I can pick what I want and change it at will.</p>

<p>I probably have 2-3tb&#39;s of data to archive and store, I also want easy
access to my data so NFS or CIFS exports are required. It wouldn&#39;t be
unfeasible to acquire a few 2 or 3 terabyte drives for my old desktop
machine which would effectively provide me with a 2 or 3 terabyte
replicated data store. Given the amount of toying around and learning
about Ceph in my spare time I would expect that Ceph would provide me
with a pretty good &ldquo;backend&rdquo; system for storing my files and the
option of &ldquo;migrating my data from one machine to another machine&rdquo; by
adding and removing OSD&#39;s. The handiest feature for me will be the
capability of expanding and shrinking the system as I need.</p>

<p>There probably aren&#39;t many people who would want to setup something
like this for a home system, but it is an alternative to the usual
RAID or LVM setup.</p>

<p>Here&#39;s my proposed setup which I&#39;m going to setup in the next few
spare weekends that I will have.</p>

<p><img class="" src="http://jcftang.github.com/downloads/images/ceph-home.png"></p>

<p>It would be great if Ceph offered some of of parity/erasure coding
instead of plain replication. I&#39;m greedy and I want to maximise my
disks that I have, I wonder how low I can go on hardware with the Ceph
software.</p>
]]></content>
    </entry>
  
</feed>
